{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9093008,"sourceType":"datasetVersion","datasetId":5463022}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Question - Answering with Retrieval\n\n본 대회의 과제는 중앙정부 재정 정보에 대한 **검색 기능**을 개선하고 활용도를 높이는 질의응답 알고리즘을 개발하는 것입니다. <br>이를 통해 방대한 재정 데이터를 일반 국민과 전문가 모두가 쉽게 접근하고 활용할 수 있도록 하는 것이 목표입니다. <br><br>\n베이스라인에서는 평가 데이터셋만을 활용하여 source pdf 마다 Vector DB를 구축한 뒤 langchain 라이브러리와 llama-2-ko-7b 모델을 사용하여 RAG 프로세스를 통해 추론하는 과정을 담고 있습니다. <br>( train_set을 활용한 훈련 과정은 포함하지 않으며, test_set  에 대한 추론만 진행합니다. )","metadata":{}},{"cell_type":"markdown","source":"# Download Library","metadata":{}},{"cell_type":"code","source":"!pip install accelerate\n!pip install -i https://pypi.org/simple/ bitsandbytes\n!pip install transformers[torch] -U\n\n!pip install datasets\n!pip install langchain\n!pip install langchain_community\n!pip install PyMuPDF\n!pip install sentence-transformers\n!pip install faiss-gpu","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:38:35.695006Z","iopub.execute_input":"2024-08-05T08:38:35.695482Z","iopub.status.idle":"2024-08-05T08:41:31.249096Z","shell.execute_reply.started":"2024-08-05T08:38:35.695414Z","shell.execute_reply":"2024-08-05T08:41:31.247365Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.32.1)\nRequirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2+cpu)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.4)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nLooking in indexes: https://pypi.org/simple/\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.3\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.42.3)\nCollecting transformers[torch]\n  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.32.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2+cpu)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\nDownloading transformers-4.43.3-py3-none-any.whl (9.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\nSuccessfully installed transformers-4.43.3\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting langchain\n  Downloading langchain-0.2.12-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.3.0,>=0.2.27 (from langchain)\n  Downloading langchain_core-0.2.28-py3-none-any.whl.metadata (6.2 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.96-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (1.33)\nCollecting packaging<25,>=23.2 (from langchain-core<0.3.0,>=0.2.27->langchain)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (4.9.0)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.7.4)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain) (2.4)\nDownloading langchain-0.2.12-py3-none-any.whl (990 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.6/990.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.28-py3-none-any.whl (379 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.9/379.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.96-py3-none-any.whl (140 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, orjson, langsmith, langchain-core, langchain-text-splitters, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.2.12 langchain-core-0.2.28 langchain-text-splitters-0.2.2 langsmith-0.1.96 orjson-3.10.6 packaging-24.1\nCollecting langchain_community\n  Downloading langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (3.9.1)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.6.7)\nRequirement already satisfied: langchain<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.2.12)\nRequirement already satisfied: langchain-core<0.3.0,>=0.2.27 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.2.28)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.1.96)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (1.26.4)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.12->langchain_community) (0.2.2)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.12->langchain_community) (2.5.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain_community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain_community) (24.1)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain_community) (4.9.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2024.7.4)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain_community) (2.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain_community) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain_community) (2.14.6)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nDownloading langchain_community-0.2.11-py3-none-any.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: langchain_community\nSuccessfully installed langchain_community-0.2.11\nCollecting PyMuPDF\n  Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\nCollecting PyMuPDFb==1.24.9 (from PyMuPDF)\n  Downloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\nDownloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\nSuccessfully installed PyMuPDF-1.24.9 PyMuPDFb-1.24.9\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.43.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.23.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.0.1\nCollecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import Library","metadata":{}},{"cell_type":"code","source":"import os\nimport unicodedata\n\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport fitz  # PyMuPDF\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    pipeline,\n    BitsAndBytesConfig\n)\nfrom accelerate import Accelerator\n\n# Langchain 관련\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.schema import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.schema.output_parser import StrOutputParser","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:41:31.251769Z","iopub.execute_input":"2024-08-05T08:41:31.252622Z","iopub.status.idle":"2024-08-05T08:41:54.677943Z","shell.execute_reply.started":"2024-08-05T08:41:31.252567Z","shell.execute_reply":"2024-08-05T08:41:54.676680Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-08-05 08:41:40.001714: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-05 08:41:40.001858: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-05 08:41:40.198670: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install kiwipiepy rank_bm25 openai tiktoken","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:41:54.680642Z","iopub.execute_input":"2024-08-05T08:41:54.681356Z","iopub.status.idle":"2024-08-05T08:42:22.639866Z","shell.execute_reply.started":"2024-08-05T08:41:54.681316Z","shell.execute_reply":"2024-08-05T08:42:22.638338Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting kiwipiepy\n  Downloading kiwipiepy-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\nCollecting rank_bm25\n  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting openai\n  Downloading openai-1.38.0-py3-none-any.whl.metadata (22 kB)\nCollecting tiktoken\n  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nCollecting kiwipiepy-model<0.19,>=0.18 (from kiwipiepy)\n  Downloading kiwipiepy_model-0.18.0.tar.gz (34.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.7/34.7 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy<2 in /opt/conda/lib/python3.10/site-packages (from kiwipiepy) (1.26.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kiwipiepy) (4.66.4)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.5.3)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.0)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from openai) (4.9.0)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\nDownloading kiwipiepy-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\nDownloading openai-1.38.0-py3-none-any.whl (335 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.9/335.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: kiwipiepy-model\n  Building wheel for kiwipiepy-model (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for kiwipiepy-model: filename=kiwipiepy_model-0.18.0-py3-none-any.whl size=34843380 sha256=f0bba1091e219a77cc0785dabf5d02298e59c2c7a61cc42d95a4bb4a7d4c0ecf\n  Stored in directory: /root/.cache/pip/wheels/0d/ea/f6/abb93f89cc196467624828ec9c29150c29a8399a589ba50bef\nSuccessfully built kiwipiepy-model\nInstalling collected packages: kiwipiepy-model, rank_bm25, kiwipiepy, tiktoken, openai\nSuccessfully installed kiwipiepy-0.18.0 kiwipiepy-model-0.18.0 openai-1.38.0 rank_bm25-0.2.2 tiktoken-0.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install konlpy","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:42:22.641491Z","iopub.execute_input":"2024-08-05T08:42:22.641885Z","iopub.status.idle":"2024-08-05T08:42:38.974884Z","shell.execute_reply.started":"2024-08-05T08:42:22.641847Z","shell.execute_reply":"2024-08-05T08:42:38.973323Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting konlpy\n  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\nCollecting JPype1>=0.7.0 (from konlpy)\n  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from konlpy) (5.2.2)\nRequirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.10/site-packages (from konlpy) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from JPype1>=0.7.0->konlpy) (24.1)\nDownloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: JPype1, konlpy\nSuccessfully installed JPype1-1.5.0 konlpy-0.6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pdfplumber","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:42:38.976676Z","iopub.execute_input":"2024-08-05T08:42:38.977166Z","iopub.status.idle":"2024-08-05T08:42:55.800055Z","shell.execute_reply.started":"2024-08-05T08:42:38.977116Z","shell.execute_reply":"2024-08-05T08:42:55.798073Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting pdfplumber\n  Downloading pdfplumber-0.11.2-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: Pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from pdfplumber) (9.5.0)\nCollecting pypdfium2>=4.18.0 (from pdfplumber)\n  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\nRequirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (41.0.7)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\nDownloading pdfplumber-0.11.2-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\nSuccessfully installed pdfminer.six-20231228 pdfplumber-0.11.2 pypdfium2-4.30.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Vector DB","metadata":{}},{"cell_type":"code","source":"from langchain.retrievers import EnsembleRetriever\nfrom langchain_community.retrievers import BM25Retriever\nfrom langchain_core.documents import Document\nfrom langchain.vectorstores import FAISS\nfrom konlpy.tag import Kkma, Okt\nfrom kiwipiepy import Kiwi\n\nkiwi = Kiwi()\nkkma = Kkma()\nokt = Okt()","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:42:55.802505Z","iopub.execute_input":"2024-08-05T08:42:55.802916Z","iopub.status.idle":"2024-08-05T08:42:59.880882Z","shell.execute_reply.started":"2024-08-05T08:42:55.802879Z","shell.execute_reply":"2024-08-05T08:42:59.879539Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def kiwi_tokenize(text):\n    return [token.form for token in kiwi.tokenize(text)]\n\ndef kkma_tokenize(text):\n    return [token for token in kkma.morphs(text)]\n\ndef okt_tokenize(text):\n    return [token for token in okt.morphs(text)]","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:42:59.882442Z","iopub.execute_input":"2024-08-05T08:42:59.882827Z","iopub.status.idle":"2024-08-05T08:42:59.890352Z","shell.execute_reply.started":"2024-08-05T08:42:59.882795Z","shell.execute_reply":"2024-08-05T08:42:59.888505Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# def process_pdf(file_path, chunk_size=1500, chunk_overlap=200):\n#     \"\"\"PDF 텍스트 추출 후 chunk 단위로 나누기\"\"\"\n#     # PDF 파일 열기\n#     doc = fitz.open(file_path)\n#     text = ''\n#     # 모든 페이지의 텍스트 추출\n#     for page in doc:\n#         text += page.get_text()\n#     # 텍스트를 chunk로 분할\n#     splitter = RecursiveCharacterTextSplitter(\n#         chunk_size=chunk_size,\n#         chunk_overlap=chunk_overlap\n#     )\n#     chunk_temp = splitter.split_text(text)\n#     # Document 객체 리스트 생성\n#     chunks = [Document(page_content=t) for t in chunk_temp]\n#     return chunks\n\nimport pdfplumber\nfrom langchain.schema import Document\n\nimport os\nimport pdfplumber\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema import Document\n\ndef process_pdf(file_path, chunk_size=200, chunk_overlap=20):\n        \"\"\"PDF를 페이지마다 청크로 나누고 메타데이터에 파일 이름 추가\"\"\"\n        # 파일 이름 추출\n        file_name = os.path.basename(file_path)\n        \n        # PDF 파일 열기\n        pdf = pdfplumber.open(file_path)\n        all_chunks = []\n        \n        # 페이지별로 처리\n        for page_number, page in enumerate(pdf.pages):\n            text = page.extract_text()\n            if text:\n                # 페이지별 텍스트 청크로 분할\n                splitter = RecursiveCharacterTextSplitter(\n                    chunk_size=chunk_size,\n                    chunk_overlap=chunk_overlap\n                )\n                chunk_temp = splitter.split_text(text)\n                \n                # Document 객체 리스트 생성 (파일 이름과 페이지 번호를 메타데이터에 포함)\n                page_chunks = [Document(page_content=t, metadata={\"Source\": file_name[:-4], \"page\": page_number}) for t in chunk_temp]\n                all_chunks.extend(page_chunks)\n        \n        pdf.close()  # PDF 파일 닫기\n        return all_chunks\n\n\ndef create_vector_db(chunks, model_path=\"jhgan/ko-sroberta-multitask\"):\n    \"\"\"FAISS DB 생성\"\"\"\n    # 임베딩 모델 설정\n    model_kwargs = {'device': 'cpu'}\n    encode_kwargs = {'normalize_embeddings': True}\n    embeddings = HuggingFaceEmbeddings(\n        model_name=model_path,\n        model_kwargs=model_kwargs,\n        encode_kwargs=encode_kwargs\n    )\n    # FAISS DB 생성 및 반환\n    db = FAISS.from_documents(chunks, embedding=embeddings)\n    return db\n\ndef normalize_path(path):\n    \"\"\"경로 유니코드 정규화\"\"\"\n    return unicodedata.normalize('NFC', path)\n\n\ndef process_pdfs_from_dataframe(df, base_directory):\n    \"\"\"딕셔너리에 pdf명을 키로해서 DB, retriever 저장\"\"\"\n    pdf_databases = {}\n    unique_paths = df['Source_path'].unique()\n    \n    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n        # 경로 정규화 및 절대 경로 생성\n        normalized_path = normalize_path(path)\n        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n        \n        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n        print(f\"Processing {pdf_title}...\")\n        \n        # PDF 처리 및 벡터 DB 생성\n        chunks = process_pdf(full_path)\n        bm25 = BM25Retriever.from_documents(chunks, search_kwargs={'k': 20})\n        kiwi_bm25 = BM25Retriever.from_documents(chunks, preprocess_func=kiwi_tokenize,  search_kwargs={'k': 20})\n        kkma_bm25 = BM25Retriever.from_documents(chunks, preprocess_func=kkma_tokenize,  search_kwargs={'k': 20})\n        okt_bm25 = BM25Retriever.from_documents(chunks, preprocess_func=okt_tokenize,  search_kwargs={'k': 20})\n        db = create_vector_db(chunks)\n        faiss = db.as_retriever(search_kwargs={'k': 20})\n        \n        # Retriever 생성\n        retriever = EnsembleRetriever(\n                    retrievers=[bm25, faiss],  # 사용할 검색 모델의 리스트\n                    weights=[0.3, 0.7],  # 각 검색 모델의 결과에 적용할 가중치\n                    search_type=\"mmr\",  # 검색 결과의 다양성을 증진시키는 MMR 방식을 사용\n                    search_kwargs={'k': 20, 'fetch_k': 20}, \n                )\n        \n        \n        # 결과 저장\n        pdf_databases[pdf_title] = {\n                'db': db,\n                'retriever': retriever\n        }\n    return pdf_databases","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:42:59.892580Z","iopub.execute_input":"2024-08-05T08:42:59.892960Z","iopub.status.idle":"2024-08-05T08:42:59.967711Z","shell.execute_reply.started":"2024-08-05T08:42:59.892922Z","shell.execute_reply":"2024-08-05T08:42:59.966457Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# DB 생성","metadata":{}},{"cell_type":"code","source":"# # Train과 Test CSV 파일 모두 로드\n# df_train = pd.read_csv('/kaggle/input/pdf-files/train.csv')\n# df_test = pd.read_csv('/kaggle/input/pdf-files/test.csv')\n\n# # 두 데이터프레임 합치기\n# df_combined = pd.concat([df_train, df_test], ignore_index=True)\n\n# # 중복된 Source_path 제거 (같은 PDF가 train과 test에 모두 있을 경우)\n# df_combined = df_combined.drop_duplicates(subset=['Source_path'])\n\n# base_directory = '/kaggle/input/pdf-files' # Your Base Directory\n# pdf_databases = process_pdfs_from_dataframe(df_combined, base_directory)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T04:27:50.040844Z","iopub.execute_input":"2024-08-05T04:27:50.041281Z","iopub.status.idle":"2024-08-05T04:27:50.047132Z","shell.execute_reply.started":"2024-08-05T04:27:50.041241Z","shell.execute_reply":"2024-08-05T04:27:50.045785Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# %pip install --upgrade --quiet  sentence-transformers > /dev/null","metadata":{"execution":{"iopub.status.busy":"2024-08-05T04:27:50.048605Z","iopub.execute_input":"2024-08-05T04:27:50.049053Z","iopub.status.idle":"2024-08-05T04:27:50.058636Z","shell.execute_reply.started":"2024-08-05T04:27:50.049020Z","shell.execute_reply":"2024-08-05T04:27:50.057543Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# base_directory = '/kaggle/input/pdf-files' # Your Base Directory\n# df = pd.read_csv('/kaggle/input/pdf-files/test.csv')\n# pdf_databases = process_pdfs_from_dataframe(df, base_directory)\n# # pdf_databases = process_pdfs_from_dataframe(df, base_directory)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T04:27:50.060131Z","iopub.execute_input":"2024-08-05T04:27:50.061048Z","iopub.status.idle":"2024-08-05T04:27:50.068571Z","shell.execute_reply.started":"2024-08-05T04:27:50.061012Z","shell.execute_reply":"2024-08-05T04:27:50.067593Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport os\n\ndef save_databases(pdf_databases, save_dir):\n    \"\"\"벡터 데이터베이스와 retriever 저장\"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    for pdf_title, data in pdf_databases.items():\n        db_path = os.path.join(save_dir, f\"{pdf_title}_db.pkl\")\n        retriever_path = os.path.join(save_dir, f\"{pdf_title}_retriever.pkl\")\n        \n        # DB 저장\n        data['db'].save_local(db_path)\n        \n        # Retriever 저장\n        with open(retriever_path, 'wb') as f:\n            pickle.dump(data['retriever'], f)\n        \n    print(f\"Databases and retrievers saved in {save_dir}\")\n    \nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\ndef load_databases(load_dir, model_path=\"jhgan/ko-sroberta-multitask\"):\n    \"\"\"저장된 벡터 데이터베이스와 retriever 로드\"\"\"\n    pdf_databases = {}\n    \n    # 임베딩 모델 설정\n    model_kwargs = {'device': 'cpu'}\n    encode_kwargs = {'normalize_embeddings': True}\n    embeddings = HuggingFaceEmbeddings(\n        model_name=model_path,\n        model_kwargs=model_kwargs,\n        encode_kwargs=encode_kwargs\n    )\n    \n    for filename in os.listdir(load_dir):\n        if filename.endswith(\"_db.pkl\"):\n            pdf_title = filename[:-7]  # Remove \"_db.pkl\"\n            db_path = os.path.join(load_dir, filename)\n            retriever_path = os.path.join(load_dir, f\"{pdf_title}_retriever.pkl\")\n            \n            # DB 로드 (allow_dangerous_deserialization 파라미터 추가)\n            db = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)\n            \n            # Retriever 로드\n            with open(retriever_path, 'rb') as f:\n                retriever = pickle.load(f)\n            \n            pdf_databases[pdf_title] = {\n                'db': db,\n                'retriever': retriever\n            }\n    \n    print(f\"Loaded {len(pdf_databases)} databases from {load_dir}\")\n    return pdf_databases\n\n\n# # # 데이터베이스 생성 후 저장\n# Train과 Test CSV 파일 모두 로드\n\n# df_test = pd.read_csv('/kaggle/input/pdf-files/train.csv')\n\n# base_directory = '/kaggle/input/pdf-files' # Your Base Directory\n# pdf_databases = process_pdfs_from_dataframe(df_test, base_directory)\n\nsave_dir = '/kaggle/working/'\n# save_databases(pdf_databases, save_dir)\n\n# 나중에 데이터베이스 로드\npdf_databases = load_databases(save_dir)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:42:59.972338Z","iopub.execute_input":"2024-08-05T08:42:59.972743Z","iopub.status.idle":"2024-08-05T08:45:35.371084Z","shell.execute_reply.started":"2024-08-05T08:42:59.972711Z","shell.execute_reply":"2024-08-05T08:45:35.369502Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n  warn_deprecated(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ac78f48d0264ea78d2b05a72ecec6f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7508e416c6ed46498920d5a2ed55e630"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.86k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56c59de5d6bf4a4c972d327735db2c1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7305a6d058a4604a6f1c818d0e45ee8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/744 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4590a52d07c946d68663aec4799f0cbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b08e907a0f4d4ba28e56db9a7699a7f2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d300e34fb874d16bc53d721dd4c2c20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee72ebc149e64975bda27609dd8a15b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ae27afbc6024c74b8d60b57a4f901b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65fd59e9d24849c091c4cf6a7725abb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"107d506b4a0b4d5bafc805b044296bc5"}},"metadata":{}},{"name":"stdout","text":"Loaded 25 databases from /kaggle/working/\n","output_type":"stream"}]},{"cell_type":"code","source":"# # save_dir = '/kaggle/working/'\n# # # save_databases(pdf_databases, save_dir)\n# import pickle\n\n# def load_databases(load_dir, model_path=\"paraphrase-multilingual-mpnet-base-v2\"):\n#     \"\"\"저장된 벡터 데이터베이스와 retriever 로드\"\"\"\n#     pdf_databases = {}\n    \n#     # 임베딩 모델 설정\n#     model_kwargs = {'device': 'cpu'}\n#     encode_kwargs = {'normalize_embeddings': True}\n#     embeddings = HuggingFaceEmbeddings(\n#         model_name=model_path,\n#         model_kwargs=model_kwargs,\n#         encode_kwargs=encode_kwargs\n#     )\n    \n#     for filename in os.listdir(load_dir):\n#         if filename.endswith(\"_db.pkl\"):\n#             pdf_title = filename[:-7]  # Remove \"_db.pkl\"\n#             db_path = os.path.join(load_dir, filename)\n#             retriever_path = os.path.join(load_dir, f\"{pdf_title}_retriever.pkl\")\n            \n#             # DB 로드 (allow_dangerous_deserialization 파라미터 추가)\n#             db = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)\n            \n#             # Retriever 로드\n#             with open(retriever_path, 'rb') as f:\n#                 retriever = pickle.load(f)\n            \n#             pdf_databases[pdf_title] = {\n#                 'db': db,\n#                 'retriever': retriever\n#             }\n    \n#     print(f\"Loaded {len(pdf_databases)} databases from {load_dir}\")\n#     return pdf_databases\n\n# save_dir = '/kaggle/working/'\n# # # 나중에 데이터베이스 로드\n# pdf_databases = load_databases(save_dir)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T01:11:21.376648Z","iopub.execute_input":"2024-08-05T01:11:21.377417Z","iopub.status.idle":"2024-08-05T01:11:21.384723Z","shell.execute_reply.started":"2024-08-05T01:11:21.377375Z","shell.execute_reply":"2024-08-05T01:11:21.383386Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# MODEL Import","metadata":{}},{"cell_type":"code","source":"# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n# from huggingface_hub import login\n\n# # 인증 토큰 설정\n# login(token='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')\n\n# def setup_llm_pipeline():\n#     # 모델 ID \n#     model_id = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n\n#     # 토크나이저 로드 및 설정\n#     tokenizer = AutoTokenizer.from_pretrained(model_id)\n#     tokenizer.use_default_system_prompt = False\n\n#     # 모델 로드\n#     model = AutoModelForCausalLM.from_pretrained(\n#         model_id,\n#         torch_dtype=torch.float16,  # 16비트 부동소수점 사용\n#         device_map=\"auto\",\n#         trust_remote_code=True )\n\n#     # HuggingFacePipeline 객체 생성\n#     text_generation_pipeline = pipeline(\n#         model=model,\n#         tokenizer=tokenizer,\n#         task=\"text-generation\",\n#         temperature=0.4,\n#         do_sample= True,\n#         return_full_text=False,\n#         max_new_tokens=512,\n#         repetition_penalty=1.2,  # 반복 억제\n#         no_repeat_ngram_size=3,  # n-gram 반복 방지\n#         num_beams=4,  # beam search 사용\n#         early_stopping=True,  \n#     )\n\n#     hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n\n#     return hf\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T01:11:21.386509Z","iopub.execute_input":"2024-08-05T01:11:21.387378Z","iopub.status.idle":"2024-08-05T01:11:21.400128Z","shell.execute_reply.started":"2024-08-05T01:11:21.387325Z","shell.execute_reply":"2024-08-05T01:11:21.398889Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# def setup_llm_pipeline():\n#     # 4비트 양자화 설정\n#     bnb_config = BitsAndBytesConfig(\n#         load_in_4bit=True,\n#         bnb_4bit_use_double_quant=True,\n#         bnb_4bit_quant_type=\"nf4\",\n#         bnb_4bit_compute_dtype=torch.bfloat16\n#     )\n\n#     # 모델 ID \n#     model_id = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n# #     token='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'\n#     # 토크나이저 로드 및 설정\n#     tokenizer = AutoTokenizer.from_pretrained(model_id)\n#     tokenizer.use_default_system_prompt = False\n\n#     # 모델 로드 및 양자화 설정 적용\n#     model = AutoModelForCausalLM.from_pretrained(\n#         model_id,\n#         quantization_config=bnb_config,\n#         device_map=\"auto\",\n#         trust_remote_code=True )\n\n#     # HuggingFacePipeline 객체 생성\n#     text_generation_pipeline = pipeline(\n#         model=model,\n#         tokenizer=tokenizer,\n#         task=\"text-generation\",\n#         temperature=0.2,\n#         do_sample= True,\n#         return_full_text=False,\n#         max_new_tokens=512,\n#         repetition_penalty=1.2,  # 반복 억제\n#         no_repeat_ngram_size=3,  # n-gram 반복 방지\n# #         num_beams=4,  # beam search 사용\n# #         early_stopping=True,  \n#     )\n\n\n#     hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n\n#     return hf","metadata":{"execution":{"iopub.status.busy":"2024-08-03T13:49:14.194967Z","iopub.execute_input":"2024-08-03T13:49:14.196079Z","iopub.status.idle":"2024-08-03T13:49:14.203153Z","shell.execute_reply.started":"2024-08-03T13:49:14.196038Z","shell.execute_reply":"2024-08-03T13:49:14.202210Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# # LLM 파이프라인\n# llm = setup_llm_pipeline()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T13:49:14.204419Z","iopub.execute_input":"2024-08-03T13:49:14.204768Z","iopub.status.idle":"2024-08-03T13:50:41.449824Z","shell.execute_reply.started":"2024-08-03T13:49:14.204735Z","shell.execute_reply":"2024-08-03T13:50:41.448866Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff698a5d4bcd46f2bb847718790189a6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n  warn_deprecated(\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install groq","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:45:35.372917Z","iopub.execute_input":"2024-08-05T08:45:35.373683Z","iopub.status.idle":"2024-08-05T08:45:51.690384Z","shell.execute_reply.started":"2024-08-05T08:45:35.373648Z","shell.execute_reply":"2024-08-05T08:45:51.688761Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Collecting groq\n  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from groq) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from groq) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from groq) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from groq) (2.5.3)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from groq) (1.3.0)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from groq) (4.9.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (3.6)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2024.7.4)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (2.14.6)\nDownloading groq-0.9.0-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: groq\nSuccessfully installed groq-0.9.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Langchain 을 이용한 추론","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/pdf-files/train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:45:51.692357Z","iopub.execute_input":"2024-08-05T08:45:51.692811Z","iopub.status.idle":"2024-08-05T08:45:51.744625Z","shell.execute_reply.started":"2024-08-05T08:45:51.692767Z","shell.execute_reply":"2024-08-05T08:45:51.743382Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#  prompt = PromptTemplate.from_template(\n#         \"\"\"You are an assistant for question-answering tasks. \n#     Use the following pieces of retrieved context to answer the question. \n#     If you don't know the answer, just say that you don't know. \n#     Answer in Korean.\n\n#     #Question: \n#     {question} \n#     #Context: \n#     {context} \n\n#     #Answer:\"\"\"\n#     )\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:45:51.746279Z","iopub.execute_input":"2024-08-05T08:45:51.746767Z","iopub.status.idle":"2024-08-05T08:45:51.752788Z","shell.execute_reply.started":"2024-08-05T08:45:51.746734Z","shell.execute_reply":"2024-08-05T08:45:51.751145Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import getpass\nimport os\n\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"GROQ_API_KEY\")\n\n\nos.environ[\"GROQ_API_KEY\"] = secret_value_0","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:45:51.754459Z","iopub.execute_input":"2024-08-05T08:45:51.754965Z","iopub.status.idle":"2024-08-05T08:45:51.954502Z","shell.execute_reply.started":"2024-08-05T08:45:51.754931Z","shell.execute_reply":"2024-08-05T08:45:51.952967Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"%pip install -qU langchain-groq","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:45:51.956294Z","iopub.execute_input":"2024-08-05T08:45:51.957128Z","iopub.status.idle":"2024-08-05T08:46:08.101669Z","shell.execute_reply.started":"2024-08-05T08:45:51.957091Z","shell.execute_reply":"2024-08-05T08:46:08.099970Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain_groq import ChatGroq\n\nllm = ChatGroq(\n    model=\"llama3-70b-8192\",\n    temperature=0.2,\n    max_tokens=1024,\n    timeout=None,\n    stop=None,\n    max_retries=2,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:46:08.103652Z","iopub.execute_input":"2024-08-05T08:46:08.104091Z","iopub.status.idle":"2024-08-05T08:46:08.695846Z","shell.execute_reply.started":"2024-08-05T08:46:08.104050Z","shell.execute_reply":"2024-08-05T08:46:08.694604Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"cnt=0","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:38:35.651888Z","iopub.execute_input":"2024-08-05T08:38:35.652267Z","iopub.status.idle":"2024-08-05T08:38:35.692599Z","shell.execute_reply.started":"2024-08-05T08:38:35.652237Z","shell.execute_reply":"2024-08-05T08:38:35.691400Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from langchain_community.document_transformers import LongContextReorder\nfrom langchain.prompts import ChatPromptTemplate\nfrom operator import itemgetter\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_core.prompts import format_document\nfrom langchain_core.prompts import ChatPromptTemplate\n\ndef normalize_string(s):\n    \"\"\"유니코드 정규화\"\"\"\n    return unicodedata.normalize('NFC', s)\n\n# 기본 문서 프롬프트를 생성합니다. (source, metadata 등을 추가할 수 있습니다)\nDEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(\n    template=\"{page_content} [source: {Source}]\"\n)\n\ndef combine_documents(\n    docs,  # 문서 목록\n    # 문서 프롬프트 (기본값: DEFAULT_DOCUMENT_PROMPT)\n    document_prompt=DEFAULT_DOCUMENT_PROMPT,\n    document_separator=\"\\n\",  # 문서 구분자 (기본값: 두 개의 줄바꿈)\n):\n    # context 에 입력으로 넣기 위한 문서 병합\n    doc_strings = [\n        f\"[{i}] {format_document(doc, document_prompt)}\" for i, doc in enumerate(docs)\n    ]  # 각 문서를 주어진 프롬프트로 포맷팅하여 문자열 목록 생성\n    return document_separator.join(\n        doc_strings\n    )  # 포맷팅된 문서 문자열을 구분자로 연결하여 반환\n\n\ndef reorder_documents(docs):\n    # 재정렬\n    reordering = LongContextReorder()\n    reordered_docs = reordering.transform_documents(docs)\n    combined = combine_documents(reordered_docs, document_separator=\"\\n\")\n    return combined\n\n\n\n\ndef format_docs(docs):\n    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n        # docs가 리스트가 아닌 경우 (예: Retriever 객체)\n    reordering = LongContextReorder()\n    reordered_docs = reordering.transform_documents(docs)\n    return \"\\n\\n\".join(doc.page_content for doc in reordered_docs)\n\nimport re\n\ndef remove_html_tags(text):\n    \"\"\"HTML 태그를 제거하는 함수\"\"\"\n    clean = re.compile('<.*?>')\n    return re.sub(clean, '', text)    \n\ndef clean_output(output):\n    # \"질문:\" 이후의 텍스트만 반환하고 HTML 태그 제거\n    if \"Answer:\" in output:\n        output = output.split(\"Answer:\")[-1].strip()\n    return remove_html_tags(output)\n\n# 결과를 저장할 리스트 초기화\nresults = []\nnormalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n# DataFrame의 각 행에 대해 처리\nfor _, row in tqdm(df[cnt:].iterrows(), total=len(df[cnt:]), desc=\"Answering Questions\"):\n    # 소스 문자열 정규화\n    source = normalize_string(row['Source'])\n    question = row['Question']\n    # 정규화된 키로 데이터베이스 검색\n    normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n    retriever = normalized_keys[source]['retriever']\n    \n    # RAG 체인 구성\n#     prompt = PromptTemplate.from_template(\n#        template = \"\"\"Given this text extracts:\n#     {context}\n\n#     -----\n#     Please answer the following question:\n#     {question}\n\n#     Answer in the following languages: {language}\n#     \"\"\"\n#     )\n    \n\n\n    prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a helpful assistant that answers question with {context}.\",\n        ),\n        (\"human\", \"Please answer the following question: {question}. Think step by step. Answer in the following languages: {language}\"),\n    ]\n    )\n\n    # RAG 체인 정의\n    rag_chain = (\n    {\n        \"context\": itemgetter(\"question\")\n        | retriever\n        | RunnableLambda(reorder_documents),  # 질문을 기반으로 문맥을 검색합니다.\n        \"question\": itemgetter(\"question\"),  # 질문을 추출합니다.\n        \"language\": itemgetter(\"language\"),  # 답변 언어를 추출합니다.\n    }\n    | prompt  # 프롬프트 템플릿에 값을 전달합니다.\n    | llm\n    | StrOutputParser()  # 모델의 출력을 문자열로 파싱합니다.\n    )\n\n    # 답변 추론\n    print(f\"Question: {question}\")\n    full_response = rag_chain.invoke({\"question\": question, \"language\": \"KOREAN\"})\n\n    # 실제 답변만 추출\n    actual_answer = clean_output(full_response)\n    print(f\"Answer: {actual_answer}\\n\")\n\n    # 결과 저장\n    results.append({\n        \"Source\": row['Source'],\n        \"Source_path\": row['Source_path'],\n        \"Question\": question,\n        \"Answer\": actual_answer  # 실제 답변만 저장\n    })\n    cnt+=1\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T08:46:08.697487Z","iopub.execute_input":"2024-08-05T08:46:08.697865Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Answering Questions:   0%|          | 0/496 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Question: 2024년 중앙정부 재정체계는 어떻게 구성되어 있나요?\n","output_type":"stream"},{"name":"stderr","text":"Answering Questions:   0%|          | 1/496 [00:00<07:55,  1.04it/s]","output_type":"stream"},{"name":"stdout","text":"Answer: 2024년 중앙정부 재정체계는 예산(일반･특별회계)과 기금으로 구분되며, 2024년 기준으로 일반회계 1개, 특별회계 21개, 기금 68개로 구성되어 있습니다.\n\nQuestion: 2024년 중앙정부의 예산 지출은 어떻게 구성되어 있나요?\n","output_type":"stream"},{"name":"stderr","text":"Answering Questions:   0%|          | 2/496 [00:02<10:52,  1.32s/it]","output_type":"stream"},{"name":"stdout","text":"Answer: Let's break down the answer step by step.\n\n2024년 중앙정부의 예산 지출은 어떻게 구성되어 있나요?\n\nAccording to the 2024 Fiscal Statistics, the 2024 central government budget expenditure is composed of:\n\n1. 의무지출 (Mandatory Expenditure): 347.4 trillion won (52.9% of total expenditure)\n2. 재량지출 (Discretionary Expenditure): 309.2 trillion won (47.1% of total expenditure)\n\nWithin the discretionary expenditure, the breakdown is:\n\n1. 보건･복지･고용 (Health, Welfare, and Employment): 122.4 trillion won (18.6% of total expenditure)\n2. 교육 (Education): 95.8 trillion won (14.6% of total expenditure)\n3. 행정안전 (Administration and Safety): 72.4 trillion won (11.0% of total expenditure)\n4. 국토교통 (Land, Infrastructure, and Transport): 60.9 trillion won (9.3% of total expenditure)\n\nAdditionally, the budget expenditure is also categorized into general accounts and special accounts. The general accounts consist of 356.5 trillion won, and the special accounts consist of 81.7 trillion won.\n\nTherefore, the 2024 central government budget expenditure is composed of mandatory expenditure, discretionary expenditure, and special accounts, with a total of 656.6 trillion won.\n\nQuestion: 기금이 예산과 다른 점은?\n","output_type":"stream"},{"name":"stderr","text":"Answering Questions:   1%|          | 3/496 [00:26<1:34:21, 11.48s/it]","output_type":"stream"},{"name":"stdout","text":"Answer: 기금이 예산과 다른 점은 다음과 같습니다.\n\n첫째, 기금은 특정 사업 운영을 위해 편성되는 재정수단입니다. 예산은 국가의 일반적 재정운영을 위한 재정수단입니다.\n\n둘째, 기금은 특정 목적을 위해 특정 자금을 운용합니다. 예산은 국가의 일반적 재정운영을 위해 다양한 수입원을 운용합니다.\n\n셋째, 기금은 일정 자금을 활용하여 특정 사업을 안정적으로 운영합니다. 예산은 국가의 일반적 재정운영을 위해 다양한 지출을 합니다.\n\n넷째, 기금은 기금관리주체가 계획안을 수립하여 운용합니다. 예산은 국회에서 심의·의결하여 운용합니다.\n\n따라서, 기금과 예산은 목적, 운용방식, 지출범위 등에서 차이가 있습니다.\n\nQuestion: 일반회계, 특별회계, 기금 간의 차이점은 무엇인가요?\n","output_type":"stream"},{"name":"stderr","text":"Answering Questions:   1%|          | 4/496 [00:58<2:42:55, 19.87s/it]","output_type":"stream"},{"name":"stdout","text":"Answer: 😊\n\n일반회계, 특별회계, 기금 간의 차이점은 다음과 같습니다.\n\n**일반회계 (General Account)**\n\n* 국가의 일반적인 재정 운영을 위한 예산\n* 일반적인 국가 운영 비용, 예산, 세입 등을 포함\n* 예산의 범위 내에서 운영\n* 국가의 기본적인 재정 기능을 수행\n\n**특별회계 (Special Account)**\n\n* 특정 사업 또는 목적을 위해 편성되는 예산\n* 특정 자금을 운용하여 특정 사업을 수행\n* 예산의 범위 내에서 운영\n* 국가의 특정 정책 또는 사업을 수행\n\n**기금 (Fund)**\n\n* 재정운영의 신축성을 기할 필요가 있을 때, 정부가 편성하고 국회에서 심의･의결한 기금운용계획에 의해 운용\n* 예산과 구분되는 재정수단\n* 특정 사업 운영 또는 특정 목적을 위해 특정 자금을 운용\n\n따라서, 일반회계는 국가의 일반적인 재정 운영을 위한 예산, 특별회계는 특정 사업 또는 목적을 위해 편성되는 예산, 기금은 재정운영의 신축성을 기할 필요가 있을 때, 정부가 편성하고 국회에서 심의･의결한 기금운용계획에 의해 운용되는 예산입니다.\n\nQuestion: 2024년 총수입은 얼마이며, 예산수입과 기금수입은 각각 몇 조원인가요?\n","output_type":"stream"},{"name":"stderr","text":"Answering Questions:   1%|          | 5/496 [01:33<3:24:59, 25.05s/it]","output_type":"stream"},{"name":"stdout","text":"Answer: Let's break down the question step by step.\n\n2024년 총수입은 얼마며, 예산수입과 기금수입은 각각 몇 조원인가요?\n\nTo find the answer, let's refer to the provided text.\n\nAccording to [1] 2024년 총수입, the total revenue for 2024 is:\n\n총수입 = 612.2조원\n\nNow, let's find the budget revenue (예산수입) and fund revenue (기금수입) separately.\n\nFrom the same section, we can see that:\n\n예산수입 = 395.5조원\n기금수입 = 216.7조원\n\nTherefore, the answer is:\n\n2024년 총수입은 612.2조원이며, 예산수입은 395.5조원, 기금수입은 216.7조원입니다.\n\nQuestion: 2024년도 예산수입은 어떻게 구성되어 있나요?\n","output_type":"stream"},{"name":"stderr","text":"Answering Questions:   1%|          | 6/496 [02:05<3:44:24, 27.48s/it]","output_type":"stream"},{"name":"stdout","text":"Answer: Let's break it down step by step. 😊\n\n2024년도 예산수입은 어떻게 구성되어 있나요?\n\nAccording to the data provided, the 2024 budget revenue is composed of:\n\n1. 일반회계 (General Account): 367.3조원 (60.0%)\n2. 특별회계 (Special Account): 28.2조원 (4.6%)\n3. 기금 (Funds): 216.7조원 (35.4%)\n\nTherefore, the 2024 budget revenue is structured as follows:\n\n* 60% from the General Account\n* 4.6% from the Special Account\n* 35.4% from Funds\n\n(Source: [19] 2024 주요 재정통계 | 2024 Fiscal Statistics)\n\nQuestion: 2024년의 기금수입은 어떻게 구성되어 있나요?\n","output_type":"stream"},{"name":"stderr","text":"Answering Questions:   1%|▏         | 7/496 [02:37<3:56:12, 28.98s/it]","output_type":"stream"},{"name":"stdout","text":"Answer: 2024년의 기금수입은 다음과 같이 구성되어 있습니다.\n\n기금수입은 216.7조원으로, 다음과 같은 구성 요소로 이루어져 있습니다.\n\n* 사회보장기여금: 92.3조원 (42.6%의 비중)\n* 융자원금회수: 33.8조원\n* 경상이전수입: 39.6조원\n\n따라서, 2024년의 기금수입은 주로 사회보장기여금이 차지하고 있습니다.\n\nQuestion: 2024년 국세수입은 얼마이며, 일반회계와 특별회계의 규모는 각각 얼마인가요?\n","output_type":"stream"},{"name":"stderr","text":"Answering Questions:   2%|▏         | 8/496 [03:12<4:10:28, 30.80s/it]","output_type":"stream"},{"name":"stdout","text":"Answer: Let's break down the question step by step.\n\n**2024년 국세수입은 얼마인가요?**\n\nTo find the answer, we need to look for the total national tax revenue in 2024.\n\nFrom [9], we can see that the total national tax revenue in 2024 is 395.5 trillion won.\n\n**일반회계와 특별회계의 규모는 각각 얼마인가요?**\n\nTo find the answer, we need to look for the breakdown of the national tax revenue into general account and special account.\n\nFrom [3], we can see that the national tax revenue in 2024 is composed of:\n\n* 일반회계 (General Account): 367.3 trillion won\n* 특별회계 (Special Account): 28.2 trillion won\n\nTherefore, the answers are:\n\n* 2024년 국세수입은 395.5조원입니다.\n* 일반회계는 367.3조원, 특별회계는 28.2조원입니다.\n\nQuestion: 2024년도 국세수입 중 일반회계 내국세수입은 몇 조원인가요?\n","output_type":"stream"},{"name":"stderr","text":"Answering Questions:   2%|▏         | 9/496 [03:48<4:23:55, 32.52s/it]","output_type":"stream"},{"name":"stdout","text":"Answer: Let's break it down step by step. 😊\n\nAccording to the provided information, we can find the answer in [10].\n\n[10] states that \"2024년 국세수입은 367.3조원이며, 일반회계 356.1조원, 특별회계 11.2조원임\".\n\nWithin the general account (일반회계), we can find the domestic tax revenue (내국세수입) which is 321.6조원 (87.6% of the national tax revenue).\n\nTherefore, the answer is:\n\n2024년도 국세수입 중 일반회계 내국세수입은 321.6조원입니다.\n\nQuestion: 2024년도 세외수입 규모와 구성은 어떤가요?\n","output_type":"stream"},{"name":"stderr","text":"Answering Questions:   2%|▏         | 10/496 [04:23<4:30:21, 33.38s/it]","output_type":"stream"},{"name":"stdout","text":"Answer: ** 2024년도 세외수입 규모는 28.2조원이며, 경상이전수입 (6.9조원, 24.5%)과 재산수입 (2.9조원, 10.2%)으로 구성되어 있습니다.\n\nQuestion: 2024년 기금수입 중 가장 큰 비중을 차지하는 항목은 무엇인가?\n","output_type":"stream"},{"name":"stderr","text":"Answering Questions:   2%|▏         | 11/496 [04:57<4:32:04, 33.66s/it]","output_type":"stream"},{"name":"stdout","text":"Answer: Let's break it down step by step. 😊\n\nAccording to the text, 2024년 기금수입 (2024 fund revenue) is composed of several items:\n\n* 사회보장기여금 (social insurance contributions)\n* 융자원금회수 (loan repayment)\n* 경상이전수입 (current transfer income)\n\nAnd the question asks, \"What is the item that takes up the largest proportion of 2024년 기금수입?\"\n\nLet's look at the text again:\n\n* 4대 사회보험성기금 기여금의 비중이 42.6%로 가장 큼 (The proportion of social insurance contributions is 42.6%, the largest)\n\nSo, the answer is: 사회보장기여금 (social insurance contributions). 👍\n\nQuestion: 2024년 총지출 기준 예산의 일반회계와 특별회계의 비중이 각각 얼마인가?\n","output_type":"stream"},{"name":"stderr","text":"Answering Questions:   2%|▏         | 12/496 [05:32<4:32:44, 33.81s/it]","output_type":"stream"},{"name":"stdout","text":"Answer: Let's find the answer step by step.\n\nFirst, we need to find the relevant information in the text. After searching, I found the relevant information in [16].\n\nAccording to [16], \"2024년 총수입은 일반회계 367.3조원(60.0%), 특별회계 28.2조원(4.6%), 기금 216.7조원(35.4%)로 구성\" which means the total expenditure in 2024 is composed of 60.0% of general account, 4.6% of special account, and 35.4% of fund.\n\nSo, the answer is:\n\n2024년 총지출 기준 예산의 일반회계와 특별회계의 비중은 각각 60.0%, 4.6%입니다.\n\nQuestion: 2024년도 총계 기준 재정규모는 얼마이며, 예산과 기금은 각각 몇 조원으로 구성되어 있는가?\n","output_type":"stream"},{"name":"stderr","text":"Answering Questions:   3%|▎         | 13/496 [06:08<4:37:29, 34.47s/it]","output_type":"stream"},{"name":"stdout","text":"Answer: **\n\nAccording to the text, the answer can be found in [13].\n\n2024년도 총계 기준 재정규모는 1,573.3조원이며, 예산 550.0조원, 기금 1,023.3조원으로 구성되어 있습니다.\n\nQuestion: 내부거래지출이란 무엇을 의미하며, 어떤 종류의 거래를 포함하고 있는가?\n","output_type":"stream"},{"name":"stderr","text":"Answering Questions:   3%|▎         | 14/496 [06:40<4:32:36, 33.93s/it]","output_type":"stream"},{"name":"stdout","text":"Answer: 😊\n\n**Step 1: Understand the term \"내부거래지출\"**\n\n\"내부거래지출\" can be broken down into two parts: \"내부거래\" and \"지출\".\n\n* \"내부거래\" means \"internal transaction\" or \"intra-governmental transaction\".\n* \"지출\" means \"expenditure\" or \"outlay\".\n\nSo, \"내부거래지출\" can be translated to \"internal transaction expenditure\" or \"intra-governmental transaction outlay\".\n\n**Step 2: Identify the types of transactions included**\n\nAccording to the provided text, 내부거래지출 includes transactions between:\n\n* 회계 (accounts) and 회계 (accounts)\n* 회계 (accounts) and 기금 (funds)\n* 기금 (funds) and 기금 (funds)\n\nThese transactions are considered internal transactions because they occur within the government or between different government agencies.\n\n**Step 3: Provide a concise answer**\n\nTherefore, 내부거래지출 refers to the expenditures or outlays resulting from internal transactions between government agencies or accounts, including transactions between accounts, between accounts and funds, and between funds.\n\nQuestion: 보전지출이란 무엇을 의미하며, 어떤 상황에서 발생하는가?\n","output_type":"stream"},{"name":"stderr","text":"Answering Questions:   3%|▎         | 15/496 [07:18<4:40:56, 35.04s/it]","output_type":"stream"},{"name":"stdout","text":"Answer: 보전지출이란 무엇을 의미하며, 어떤 상황에서 발생하는가?\n\n보전지출은 회계 또는 기금의 민간차입 상환(국채상환), 남은 자금의 금융기관 예치(기금여유자금 운용) 등과 같은 경우에 발생하는 지출을 의미합니다.\n\n보전지출이 발생하는 상황은 다음과 같습니다.\n\n1. 민간차입 상환: 정부가 민간에서 차입한 자금을 상환하는 경우에 발생합니다.\n2. 남은 자금의 금융기관 예치: 정부가 예산에서 남은 자금을 금융기관에 예치하여 운용하는 경우에 발생합니다.\n3. 기금여유자금 운용: 기금의 여유자금을 운용하여 이자수입을 얻는 경우에 발생합니다.\n\n따라서, 보전지출은 정부의 재정운용 과정에서 발생하는 예상치 못한 지출이나 예치, 운용 등의 경우에 발생하는 지출을 의미합니다.\n\nQuestion: 2024년에 일반회계의 총지출은 얼마이며, 중앙정부 총지출 대비 어느 정도의 비율을 차지하는가?\n","output_type":"stream"}]},{"cell_type":"code","source":"results['Answer']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.schema import Document\nfrom langchain_community.document_transformers import LongContextReorder\nimport unicodedata\nimport re\n\ndef normalize_string(s):\n    \"\"\"유니코드 정규화\"\"\"\n    return unicodedata.normalize('NFC', s)\n\ndef format_docs(docs):\n    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅하고 소스를 포함\"\"\"\n    reordering = LongContextReorder()\n    reordered_docs = reordering.transform_documents(docs)\n    return \"\\n\\n\".join(f\"Source: {source}\\n{doc.page_content}\" for doc in reordered_docs)\n\ndef remove_html_tags(text):\n    \"\"\"HTML 태그를 제거하는 함수\"\"\"\n    clean = re.compile('<.*?>')\n    return re.sub(clean, '', text)\n\ndef clean_output(output):\n    \"\"\"\"질문:\" 이후의 텍스트만 반환하고 HTML 태그 제거\"\"\"\n    if \"답변만 작성하세요:\" in output:\n        output = output.split(\"답변만 작성하세요:\")[-1].strip()\n    return remove_html_tags(output)\n\n# 결과를 저장할 리스트 초기화\nresults = []\nnormalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n\n# DataFrame의 각 행에 대해 처리\nfor _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n    # 소스 문자열 정규화\n    source = normalize_string(row['Source'])\n    question = row['Question']\n    print(question, '요게 질문')\n\n    # 정규화된 키로 데이터베이스 검색\n    retriever = normalized_keys[source]['retriever']\n    \n    # RAG 체인 구성\n    prompt = PromptTemplate.from_template(\n    \"\"\"당신은 사용자들의 질문과 문맥을 받아 답변을 도와주는 지능형 어시스턴트입니다. \n    반드시 다음의 문맥 조각들만 사용하여 질문에 답변하세요. 단계별로 생각한 후 답변하세요.\n\n    답변을 가짜로 만들어내지 마세요:\n     - 만약 문맥에서 질문의 답을 결정할 수 없다면 \"그 질문에 대한 답을 결정할 수 없습니다.\"라고 하세요.\n     - 문맥이 비어 있으면 \"그 질문에 대한 답을 모릅니다.\"라고 하세요.\n\n    답변은 반드시 한국어로 하세요. 설명은 필요 없습니다.\n    \n    예시 1:\n    질문 : 2024년도 국세수입 중 일반회계 내국세수입은 몇 조원인가요?\n    답변 : 2024년도 일반회계 내국세수입은 321.6조원입니다.\n    \n    예시 2:\n    질문 : 2024년도 세외수입 규모와 구성은 어떤가요?\n    답변 : 2024년 세외수입은 일반회계에서 11.2조원, 특별회계에서 17.0조원으로 나타났습니다.\n\n\n    #문맥: \n    {context}\n\n    #질문:\n    {question}\n\n    #답변만 작성하세요:\"\"\"\n)\n\n\n    # RAG 체인 정의\n    rag_chain = (\n    {\n        \"context\": itemgetter(\"question\")\n        | faiss\n        | RunnableLambda(reorder_documents),  # 질문을 기반으로 문맥을 검색합니다.\n        \"question\": itemgetter(\"question\"),  # 질문을 추출합니다.\n        \"language\": itemgetter(\"language\"),  # 답변 언어를 추출합니다.\n    }\n    | prompt  # 프롬프트 템플릿에 값을 전달합니다.\n#     | ChatOpenAI()  # 언어 모델에 프롬프트를 전달합니다.\n#     | StrOutputParser()  # 모델의 출력을 문자열로 파싱합니다.\n)\n\n    # 답변 추론\n    print(f\"Question: {question}\")\n    full_response = rag_chain.invoke(question)\n\n    # 실제 답변만 추출\n    actual_answer = clean_output(full_response)\n    print(f\"Answer: {actual_answer}\\n\")\n\n    # 결과 저장\n    results.append({\n        \"Source\": row['Source'],\n        \"Source_path\": row['Source_path'],\n        \"Question\": question,\n        \"Answer\": actual_answer  # 실제 답변만 저장\n    })\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# 제출용 샘플 파일 로드\nsubmit_df = pd.read_csv(\"/kaggle/input/pdf-files/sample_submission.csv\")\n\n# 생성된 답변을 제출 DataFrame에 추가\nsubmit_df['Answer'] = [item['Answer'] for item in results]\nsubmit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n\n# 결과를 CSV 파일로 저장\nsubmit_df.to_csv(\"./37_train_70b.csv\", encoding='UTF-8-sig', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T06:07:45.113014Z","iopub.execute_input":"2024-08-05T06:07:45.113443Z","iopub.status.idle":"2024-08-05T06:07:46.277690Z","shell.execute_reply.started":"2024-08-05T06:07:45.113407Z","shell.execute_reply":"2024-08-05T06:07:46.275777Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m submit_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/pdf-files/sample_submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 생성된 답변을 제출 DataFrame에 추가\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43msubmit_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAnswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[1;32m      6\u001b[0m submit_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m submit_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m데이콘\u001b[39m\u001b[38;5;124m\"\u001b[39m)     \u001b[38;5;66;03m# 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 결과를 CSV 파일로 저장\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4517\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4530\u001b[0m     ):\n\u001b[1;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Length of values (496) does not match length of index (98)"],"ename":"ValueError","evalue":"Length of values (496) does not match length of index (98)","output_type":"error"}]},{"cell_type":"code","source":"submit_df = pd.read_csv(\"/kaggle/input/pdf-files/train.csv\")\n\nsubmit_df['Answer'] = [item['Answer'] for item in results]\nsubmit_df.to_csv(\"./37_train_8b.csv\", encoding='UTF-8-sig', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T06:19:44.417868Z","iopub.execute_input":"2024-08-05T06:19:44.419244Z","iopub.status.idle":"2024-08-05T06:19:44.469637Z","shell.execute_reply.started":"2024-08-05T06:19:44.419199Z","shell.execute_reply":"2024-08-05T06:19:44.467314Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\n\ndf = pd.read_csv('/kaggle/working/37_train_8b.csv')\npred = df['Answer']\n\ndf = pd.read_csv('/kaggle/input/pdf-files/train.csv')\ngt = df['Answer']\n\ndef calculate_f1_score(true_sentence, predicted_sentence, sum_mode=True):\n    true_counter = Counter(true_sentence)\n    predicted_counter = Counter(predicted_sentence)\n\n    #문자가 등장한 개수도 고려\n    if sum_mode:\n        true_positive = sum((true_counter & predicted_counter).values())\n        predicted_positive = sum(predicted_counter.values())\n        actual_positive = sum(true_counter.values())\n\n    #문자 자체가 있는 것에 focus를 맞춤\n    else:\n        true_positive = len((true_counter & predicted_counter).values())\n        predicted_positive = len(predicted_counter.values())\n        actual_positive = len(true_counter.values())\n\n    #f1 score 계산\n    precision = true_positive / predicted_positive if predicted_positive > 0 else 0\n    recall = true_positive / actual_positive if actual_positive > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    return precision, recall, f1_score\n\ndef calculate_average_f1_score(true_sentences, predicted_sentences):\n    \n    total_precision = 0\n    total_recall = 0\n    total_f1_score = 0\n    \n    for true_sentence, predicted_sentence in zip(true_sentences, predicted_sentences):\n        precision, recall, f1_score = calculate_f1_score(true_sentence, predicted_sentence)\n        total_precision += precision\n        total_recall += recall\n        total_f1_score += f1_score\n    \n    avg_precision = total_precision / len(true_sentences)\n    avg_recall = total_recall / len(true_sentences)\n    avg_f1_score = total_f1_score / len(true_sentences)\n    \n    return {\n        'average_precision': avg_precision,\n        'average_recall': avg_recall,\n        'average_f1_score': avg_f1_score\n    }\n\nresult = calculate_average_f1_score(gt, pred)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T06:20:05.541716Z","iopub.execute_input":"2024-08-05T06:20:05.542387Z","iopub.status.idle":"2024-08-05T06:20:05.661707Z","shell.execute_reply.started":"2024-08-05T06:20:05.542334Z","shell.execute_reply":"2024-08-05T06:20:05.659442Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"{'average_precision': 0.1829195502115584, 'average_recall': 0.8319427170235097, 'average_f1_score': 0.2666576168131044}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}