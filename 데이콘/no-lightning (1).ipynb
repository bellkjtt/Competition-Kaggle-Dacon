{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8106402,"sourceType":"datasetVersion","datasetId":4787916}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport pytorch_lightning as L\n\nfrom glob import glob\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom torchvision.io import read_image\nfrom torchvision.transforms import v2 as  transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Swinv2Config, Swinv2Model, AutoImageProcessor, AutoModelForImageClassification\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-20T12:12:39.513376Z","iopub.execute_input":"2024-04-20T12:12:39.514089Z","iopub.status.idle":"2024-04-20T12:12:59.582310Z","shell.execute_reply.started":"2024-04-20T12:12:39.514054Z","shell.execute_reply":"2024-04-20T12:12:59.581509Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-20 12:12:51.826616: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-20 12:12:51.826707: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-20 12:12:51.924096: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"device_ids = [i for i in range(torch.cuda.device_count())]\nprint(f\"Using {len(device_ids)} GPUs\")","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:12:59.584176Z","iopub.execute_input":"2024-04-20T12:12:59.585450Z","iopub.status.idle":"2024-04-20T12:12:59.628034Z","shell.execute_reply.started":"2024-04-20T12:12:59.585412Z","shell.execute_reply":"2024-04-20T12:12:59.627152Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using 2 GPUs\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch # 파이토치 \nimport random\nimport numpy as np\nimport os\n\n# 시드값 고정\ndef seed_everything(seed):\n    random.seed(seed) ##random module의 시드 고정\n    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\"\n    os.environ['PYTHONHASHSEED'] = str(seed) #해시 함수의 랜덤성 제어, 자료구조 실행할 때 동일한 순서 고정\n    np.random.seed(seed) #numpy 랜덤 숫자 일정\n    torch.manual_seed(seed) # torch라이브러리에서 cpu 텐서 생성 랜덤 시드 고정\n    torch.cuda.manual_seed(seed) # cuda의 gpu텐서에 대한 시드 고정\n    torch.backends.cudnn.deterministic = True # 백엔드가 결정적 알고리즘만 사용하도록 고정 \n    torch.backends.cudnn.benchmark = True # CuDNN이 여러 내부 휴리스틱을 사용하여 가장 빠른 알고리즘 동적으로 찾도록 설정","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:12:59.628988Z","iopub.execute_input":"2024-04-20T12:12:59.629268Z","iopub.status.idle":"2024-04-20T12:12:59.635658Z","shell.execute_reply.started":"2024-04-20T12:12:59.629244Z","shell.execute_reply":"2024-04-20T12:12:59.634721Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"seed_everything(42)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:12:59.637900Z","iopub.execute_input":"2024-04-20T12:12:59.638184Z","iopub.status.idle":"2024-04-20T12:12:59.644614Z","shell.execute_reply.started":"2024-04-20T12:12:59.638159Z","shell.execute_reply":"2024-04-20T12:12:59.643696Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:12:59.645697Z","iopub.execute_input":"2024-04-20T12:12:59.645950Z","iopub.status.idle":"2024-04-20T12:12:59.653749Z","shell.execute_reply.started":"2024-04-20T12:12:59.645928Z","shell.execute_reply":"2024-04-20T12:12:59.652873Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/brid-image/train.csv')\ntrain_df['img_path'] = train_df['img_path'].apply(lambda x: os.path.join('/kaggle/input/brid-image', x))\ntrain_df['upscale_img_path'] = train_df['upscale_img_path'].apply(lambda x: os.path.join('/kaggle/input/brid-image', x))\nle = LabelEncoder()\ntrain_df['class'] = le.fit_transform(train_df['label'])","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:12:59.654752Z","iopub.execute_input":"2024-04-20T12:12:59.654974Z","iopub.status.idle":"2024-04-20T12:12:59.782579Z","shell.execute_reply.started":"2024-04-20T12:12:59.654954Z","shell.execute_reply":"2024-04-20T12:12:59.781674Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if not len(train_df) == len(os.listdir('/kaggle/input/brid-image/train')):\n    raise ValueError()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:12:59.783723Z","iopub.execute_input":"2024-04-20T12:12:59.784068Z","iopub.status.idle":"2024-04-20T12:13:00.152366Z","shell.execute_reply.started":"2024-04-20T12:12:59.784036Z","shell.execute_reply":"2024-04-20T12:13:00.151385Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:13:00.153662Z","iopub.execute_input":"2024-04-20T12:13:00.154388Z","iopub.status.idle":"2024-04-20T12:13:00.172611Z","shell.execute_reply.started":"2024-04-20T12:13:00.154353Z","shell.execute_reply":"2024-04-20T12:13:00.171724Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                               img_path  \\\n0      /kaggle/input/brid-image/./train/TRAIN_00000.jpg   \n1      /kaggle/input/brid-image/./train/TRAIN_00001.jpg   \n2      /kaggle/input/brid-image/./train/TRAIN_00002.jpg   \n3      /kaggle/input/brid-image/./train/TRAIN_00003.jpg   \n4      /kaggle/input/brid-image/./train/TRAIN_00004.jpg   \n...                                                 ...   \n15829  /kaggle/input/brid-image/./train/TRAIN_15829.jpg   \n15830  /kaggle/input/brid-image/./train/TRAIN_15830.jpg   \n15831  /kaggle/input/brid-image/./train/TRAIN_15831.jpg   \n15832  /kaggle/input/brid-image/./train/TRAIN_15832.jpg   \n15833  /kaggle/input/brid-image/./train/TRAIN_15833.jpg   \n\n                                        upscale_img_path              label  \\\n0      /kaggle/input/brid-image/./upscale_train/TRAIN...     Ruddy Shelduck   \n1      /kaggle/input/brid-image/./upscale_train/TRAIN...       Gray Wagtail   \n2      /kaggle/input/brid-image/./upscale_train/TRAIN...     Indian Peacock   \n3      /kaggle/input/brid-image/./upscale_train/TRAIN...  Common Kingfisher   \n4      /kaggle/input/brid-image/./upscale_train/TRAIN...  Common Kingfisher   \n...                                                  ...                ...   \n15829  /kaggle/input/brid-image/./upscale_train/TRAIN...        Common Myna   \n15830  /kaggle/input/brid-image/./upscale_train/TRAIN...  Common Kingfisher   \n15831  /kaggle/input/brid-image/./upscale_train/TRAIN...       Cattle Egret   \n15832  /kaggle/input/brid-image/./upscale_train/TRAIN...     Ruddy Shelduck   \n15833  /kaggle/input/brid-image/./upscale_train/TRAIN...     Forest Wagtail   \n\n       class  \n0         19  \n1          9  \n2         13  \n3          3  \n4          3  \n...      ...  \n15829      4  \n15830      3  \n15831      2  \n15832     19  \n15833      8  \n\n[15834 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>img_path</th>\n      <th>upscale_img_path</th>\n      <th>label</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/brid-image/./train/TRAIN_00000.jpg</td>\n      <td>/kaggle/input/brid-image/./upscale_train/TRAIN...</td>\n      <td>Ruddy Shelduck</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/brid-image/./train/TRAIN_00001.jpg</td>\n      <td>/kaggle/input/brid-image/./upscale_train/TRAIN...</td>\n      <td>Gray Wagtail</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/brid-image/./train/TRAIN_00002.jpg</td>\n      <td>/kaggle/input/brid-image/./upscale_train/TRAIN...</td>\n      <td>Indian Peacock</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/brid-image/./train/TRAIN_00003.jpg</td>\n      <td>/kaggle/input/brid-image/./upscale_train/TRAIN...</td>\n      <td>Common Kingfisher</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/brid-image/./train/TRAIN_00004.jpg</td>\n      <td>/kaggle/input/brid-image/./upscale_train/TRAIN...</td>\n      <td>Common Kingfisher</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15829</th>\n      <td>/kaggle/input/brid-image/./train/TRAIN_15829.jpg</td>\n      <td>/kaggle/input/brid-image/./upscale_train/TRAIN...</td>\n      <td>Common Myna</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>15830</th>\n      <td>/kaggle/input/brid-image/./train/TRAIN_15830.jpg</td>\n      <td>/kaggle/input/brid-image/./upscale_train/TRAIN...</td>\n      <td>Common Kingfisher</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>15831</th>\n      <td>/kaggle/input/brid-image/./train/TRAIN_15831.jpg</td>\n      <td>/kaggle/input/brid-image/./upscale_train/TRAIN...</td>\n      <td>Cattle Egret</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>15832</th>\n      <td>/kaggle/input/brid-image/./train/TRAIN_15832.jpg</td>\n      <td>/kaggle/input/brid-image/./upscale_train/TRAIN...</td>\n      <td>Ruddy Shelduck</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>15833</th>\n      <td>/kaggle/input/brid-image/./train/TRAIN_15833.jpg</td>\n      <td>/kaggle/input/brid-image/./upscale_train/TRAIN...</td>\n      <td>Forest Wagtail</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n<p>15834 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"SEED = 42\nN_SPLIT = 5\nBATCH_SIZE = 24","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:13:00.173692Z","iopub.execute_input":"2024-04-20T12:13:00.174026Z","iopub.status.idle":"2024-04-20T12:13:00.178202Z","shell.execute_reply.started":"2024-04-20T12:13:00.173994Z","shell.execute_reply":"2024-04-20T12:13:00.177205Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=N_SPLIT, random_state=SEED, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:13:00.181957Z","iopub.execute_input":"2024-04-20T12:13:00.182265Z","iopub.status.idle":"2024-04-20T12:13:00.186830Z","shell.execute_reply.started":"2024-04-20T12:13:00.182235Z","shell.execute_reply":"2024-04-20T12:13:00.185972Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 데이터셋 정의","metadata":{}},{"cell_type":"code","source":"import cv2\nfrom torch.utils.data import Dataset # 데이터 생성을 위한 클래스\nimport numpy as np\n\nclass ImageDataset(Dataset):\n    # 초기화 메서드(생성자)\n    def __init__(self, df, path_col,  mode='train',transform=None):\n        super().__init__() # 상속받은 Dataset의 __init__() 메서드 호출\n        self.df = df\n        self.path_col = path_col\n        self.mode = mode\n        self.transform = transform\n        \n    # 데이터셋 크기 반환 메서드 \n    def __len__(self):\n        return len(self.df)\n    \n    # 인덱스(idx)에 해당하는 데이터 반환 메서드\n    def __getitem__(self, idx):\n        if self.mode == 'train':\n            row = self.df.iloc[idx]   # 이미지 ID\n            img_path = row[self.path_col]\n            image = cv2.imread(img_path)              # 이미지 파일 읽기\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # 이미지 색상 보정\n            label = row['class']\n            # 이미지 변환 \n            if self.transform is not None:\n                image = self.transform(image=image)['image']\n            # 테스트 데이터면 이미지 데이터만 반환, 그렇지 않으면 타깃값도 반환 \n                return image,label\n            else:\n                return image\n        \n        elif self.mode == 'val':\n            row = self.df.iloc[idx]   # 이미지 ID\n            img_path = row[self.path_col]\n            image = cv2.imread(img_path)              # 이미지 파일 읽기\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # 이미지 색상 보정\n            label = row['class']\n            \n            if self.transform is not None:\n                image = self.transform(image=image)['image']\n            # 테스트 데이터면 이미지 데이터만 반환, 그렇지 않으면 타깃값도 반환 \n                return image,label\n            else:\n                return image","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:13:00.187771Z","iopub.execute_input":"2024-04-20T12:13:00.188024Z","iopub.status.idle":"2024-04-20T12:13:00.200722Z","shell.execute_reply.started":"2024-04-20T12:13:00.188001Z","shell.execute_reply":"2024-04-20T12:13:00.199835Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_transform = A.Compose([\n    A.Resize(256, 256, interpolation=cv2.INTER_CUBIC),  # 이미지 크기를 조정\n    A.RandomBrightnessContrast(brightness_limit=0.2, # 밝기 대비 조절\n                               contrast_limit=0.2, p=0.3),\n    A.VerticalFlip(p=0.2),    # 상하 대칭 변환\n    A.HorizontalFlip(p=0.5),  # 좌우 대칭 변환 \n    A.ShiftScaleRotate(       # 이동, 스케일링, 회전 변환\n        shift_limit=0.1,\n        scale_limit=0.2,\n        rotate_limit=30, p=0.3),\n    A.OneOf([A.Emboss(p=1),   # 양각화, 날카로움, 블러 효과\n             A.Sharpen(p=1),\n             A.Blur(p=1)], p=0.3),\n    A.PiecewiseAffine(p=0.3), # 어파인 변환 \n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225),max_pixel_value=255.0, always_apply=False, p=1.0),  # 정규화\n    ToTensorV2()   \n])\n\n# 검증용 변환은 데이터 증강을 포함하지 않고 기본 전처리만 포함합니다.\nval_transform = A.Compose([\n    A.Resize(256, 256, interpolation=cv2.INTER_CUBIC),  # 이미지 크기를 조정\n#     transforms.CenterCrop(128),  # 중앙에서 크롭\n              # 텐서로 변환\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225),max_pixel_value=255.0, always_apply=False, p=1.0),  # 정규화\n    ToTensorV2()   \n])","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:13:00.201773Z","iopub.execute_input":"2024-04-20T12:13:00.202017Z","iopub.status.idle":"2024-04-20T12:13:00.212798Z","shell.execute_reply.started":"2024-04-20T12:13:00.201987Z","shell.execute_reply":"2024-04-20T12:13:00.211908Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_dir = 'img_path'\n\ndataset_train = ImageDataset(train_df, path_col=img_dir, transform=train_transform)\ndataset_valid = ImageDataset(train_df, path_col=img_dir, transform=val_transform)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:13:00.213786Z","iopub.execute_input":"2024-04-20T12:13:00.214280Z","iopub.status.idle":"2024-04-20T12:13:00.222329Z","shell.execute_reply.started":"2024-04-20T12:13:00.214257Z","shell.execute_reply":"2024-04-20T12:13:00.221578Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    \ng = torch.Generator()\ng.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:13:00.223519Z","iopub.execute_input":"2024-04-20T12:13:00.223812Z","iopub.status.idle":"2024-04-20T12:13:00.232234Z","shell.execute_reply.started":"2024-04-20T12:13:00.223769Z","shell.execute_reply":"2024-04-20T12:13:00.231254Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x78d13f916110>"},"metadata":{}}]},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, model):\n        super(CustomModel, self).__init__()\n        self.model = model\n        self.clf = nn.Sequential(\n            nn.Tanh(),\n            nn.LazyLinear(25),\n        )\n        \n\n#     @torch.compile\n    def forward(self, x):\n        x = self.model(x).pooler_output\n        x = self.clf(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:13:00.233304Z","iopub.execute_input":"2024-04-20T12:13:00.233546Z","iopub.status.idle":"2024-04-20T12:13:00.239770Z","shell.execute_reply.started":"2024-04-20T12:13:00.233524Z","shell.execute_reply":"2024-04-20T12:13:00.238729Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n\nearly_stopping = EarlyStopping(\n    monitor='val_macro_f1',\n    min_delta=0.01,\n    patience=3,\n    verbose=True,\n    mode='max'\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:13:00.240961Z","iopub.execute_input":"2024-04-20T12:13:00.241320Z","iopub.status.idle":"2024-04-20T12:13:00.259810Z","shell.execute_reply.started":"2024-04-20T12:13:00.241293Z","shell.execute_reply":"2024-04-20T12:13:00.259172Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"learning_rate = 8e-6\nnum_epochs = 3","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:13:00.260639Z","iopub.execute_input":"2024-04-20T12:13:00.260872Z","iopub.status.idle":"2024-04-20T12:13:00.264667Z","shell.execute_reply.started":"2024-04-20T12:13:00.260851Z","shell.execute_reply":"2024-04-20T12:13:00.263709Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn # 신경망 모듈\n\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:13:00.265705Z","iopub.execute_input":"2024-04-20T12:13:00.265987Z","iopub.status.idle":"2024-04-20T12:13:00.271096Z","shell.execute_reply.started":"2024-04-20T12:13:00.265958Z","shell.execute_reply":"2024-04-20T12:13:00.270106Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from transformers import get_cosine_schedule_with_warmup\n\n# 스케줄러 생성","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:13:00.272178Z","iopub.execute_input":"2024-04-20T12:13:00.272449Z","iopub.status.idle":"2024-04-20T12:13:00.286737Z","shell.execute_reply.started":"2024-04-20T12:13:00.272427Z","shell.execute_reply":"2024-04-20T12:13:00.286031Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import torch\n\n\nclass SAM(torch.optim.Optimizer):\n    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n\n        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n        super(SAM, self).__init__(params, defaults)\n\n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n        self.param_groups = self.base_optimizer.param_groups\n        self.defaults.update(self.base_optimizer.defaults)\n\n    @torch.no_grad()\n    def first_step(self, zero_grad=False):\n        grad_norm = self._grad_norm()\n        for group in self.param_groups:\n            scale = group[\"rho\"] / (grad_norm + 1e-12)\n\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                self.state[p][\"old_p\"] = p.data.clone()\n                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def second_step(self, zero_grad=False):\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n\n        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n\n        self.first_step(zero_grad=True)\n        closure()\n        self.second_step()\n\n    def _grad_norm(self):\n        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n        norm = torch.norm(\n                    torch.stack([\n                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n                        for group in self.param_groups for p in group[\"params\"]\n                        if p.grad is not None\n                    ]),\n                    p=2\n               )\n        return norm\n\n    def load_state_dict(self, state_dict):\n        super().load_state_dict(state_dict)\n        self.base_optimizer.param_groups = self.param_groups","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:13:00.287846Z","iopub.execute_input":"2024-04-20T12:13:00.288096Z","iopub.status.idle":"2024-04-20T12:13:00.302604Z","shell.execute_reply.started":"2024-04-20T12:13:00.288074Z","shell.execute_reply":"2024-04-20T12:13:00.301664Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"for fold_idx, (train_index, val_index) in enumerate(skf.split(train_df, train_df['class'])):\n\n    train_fold_df = train_df.loc[train_index,:]\n    val_fold_df = train_df.loc[val_index,:]\n    \n    dataset_train = ImageDataset(train_fold_df, path_col=img_dir, transform=train_transform)\n    dataset_valid = ImageDataset(val_fold_df, path_col=img_dir, transform=val_transform)\n    print(dataset_train,dataset_valid)\n    \n    loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, \n                          shuffle=True, worker_init_fn=seed_worker,\n                          generator=g, num_workers=2)\n    loader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE, \n                          shuffle=False, worker_init_fn=seed_worker,\n                          generator=g, num_workers=2)\n    \n    \n    \n    model = Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\")\n    model = CustomModel(model).cuda()\n    \n    \n    model.to(device)\n    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n    outputs = model(dummy_input)\n    \n    if torch.cuda.device_count() > 1:\n        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n      # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n        model = nn.DataParallel(model,device_ids=device_ids).to(device)\n    \n    print(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    scheduler = get_cosine_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=len(loader_train)//10, \n                                            num_training_steps=len(loader_train)*num_epochs)\n    \n    checkpoint_callback = ModelCheckpoint(\n    monitor='val_macro_f1',\n    mode='max',\n    dirpath='./kaggle/working/',\n    filename=f'swinv2-large-resize-fold_idx={fold_idx}'+'-{epoch:02d}-{train_loss:.4f}-{val_score:.4f}',\n    save_top_k=1,\n    save_weights_only=False,\n    verbose=True\n)\n    best_macro_f1=0\n    \n    for epoch in range(num_epochs):\n        \n        model.train()\n        train_loss = 0\n        if epoch == 0:\n            checkpoint_callback.on_validation_epoch_end(epoch, {'val_loss': np.Inf, 'val_macro_f1': 0})\n        \n        # train_loader 반복 시 tqdm 적용\n        train_loop = tqdm(loader_train, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\", leave=False)\n        for images, labels in train_loop:\n            images = images.float().to(device)\n            labels = labels.long().to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            train_loss += loss.item()\n            preds = outputs.argmax(dim=1)\n            train_loop.set_postfix(loss=loss.item())\n            \n        train_loss /= len(loader_train)\n        print(f'에폭 [{epoch+1}/{num_epochs}] - 훈련 데이터 손실값 : {train_loss:.4f}')    \n        \n        # == [ 검증 ] ==============================================\n        model.eval()\n        val_loss = 0\n        y_true = []\n        y_pred = []\n        \n        \n        with torch.no_grad():\n            val_loop = tqdm(loader_valid, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\", leave=False)\n            for images, labels in val_loop:\n                images = images.float().to(device)\n                labels = labels.long().to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n\n                val_loss += loss.item()\n\n                preds = outputs.argmax(dim=1)\n                val_loop.set_postfix(loss=loss.item())\n\n\n                y_true.extend(labels.cpu().numpy())\n                y_pred.extend(preds.cpu().numpy())\n\n        val_loss /= len(loader_valid)\n\n        macro_f1 = f1_score(y_true, y_pred, average='macro')\n\n        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, val loss: {val_loss:.4f}, val_macro_f1: {macro_f1:.4f}')\n    \n        early_stopping.on_validation_epoch_end(epoch, {'val_loss': val_loss, 'val_macro_f1': macro_f1})\n        checkpoint_callback.on_validation_epoch_end(epoch, {'val_loss': val_loss, 'val_macro_f1': macro_f1})\n        \n        if macro_f1>best_macro_f1:\n            best_model_path = f'/kaggle/working/swinv2-large-resize-fold_idx={fold_idx}'+ f'-{epoch:2d}-{train_loss:.4f}-{macro_f1:.4f}'\n            torch.save(model.state_dict(), best_model_path)\n            best_macro_f1 = macro_f1\n    \n    model.cpu()\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:13:00.303883Z","iopub.execute_input":"2024-04-20T12:13:00.304255Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"<__main__.ImageDataset object at 0x78d13e3d3280> <__main__.ImageDataset object at 0x78d13e3d36a0>\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc8bcb275f2e4bf98e4e8dc5ea509f2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/787M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"812a2ac40c8e4fa793c4e28616f260b2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n  warnings.warn('Lazy modules are a new feature under heavy development '\n","output_type":"stream"},{"name":"stdout","text":"Let's use 2 GPUs!\ncuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/3 - Training:   0%|          | 0/528 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6f43011a64a46e7bf24478d2e32638b"}},"metadata":{}}]},{"cell_type":"code","source":"model.cpu()\ndel model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}