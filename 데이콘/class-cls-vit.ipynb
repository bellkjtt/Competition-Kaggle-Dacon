{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8152424,"sourceType":"datasetVersion","datasetId":4821602,"isSourceIdPinned":true}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import Resize, Normalize, Compose\nfrom transformers import ViTForImageClassification, ViTConfig, ViTImageProcessor\nfrom torchvision.transforms import ToTensor, Resize, Normalize, Compose\nfrom tqdm import tqdm  # tqdm import\nfrom torch import nn\nfrom sklearn.metrics import f1_score","metadata":{"execution":{"iopub.status.busy":"2024-04-18T08:00:01.257533Z","iopub.execute_input":"2024-04-18T08:00:01.257981Z","iopub.status.idle":"2024-04-18T08:00:26.238408Z","shell.execute_reply.started":"2024-04-18T08:00:01.257948Z","shell.execute_reply":"2024-04-18T08:00:26.237046Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-18 08:00:12.752207: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-18 08:00:12.752396: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-18 08:00:12.928447: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n\nearly_stopping = EarlyStopping(\n    monitor='val_macro_f1',\n    min_delta=0.01,\n    patience=3,\n    verbose=True,\n    mode='max'\n)\n\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_macro_f1',\n    dirpath='./kaggle/working/',\n    filename='vit-model-{epoch:02d}-{val_macro_f1:.4f}',\n    save_top_k=3,\n    mode='max',\n)\n\nlr_reducer = ReduceLROnPlateau(\n    monitor='val_macro_f1',\n    factor=0.3,\n    patience=3,\n    verbose=True\n)\n\ncallbacks = [early_stopping, checkpoint_callback, lr_reducer]","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:46:56.208435Z","iopub.execute_input":"2024-04-18T05:46:56.209319Z","iopub.status.idle":"2024-04-18T05:47:00.707275Z","shell.execute_reply.started":"2024-04-18T05:46:56.209278Z","shell.execute_reply":"2024-04-18T05:47:00.706285Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/dataset/project (2)'\nbatch_size = 32\nnum_workers = 4\n\n# Define the transforms\ntransform = Compose([\n    Resize((224, 224)),\n    ToTensor(),  # PIL 이미지를 Tensor로 변환\n    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])\n\n# Load the dataset\ntrain_dataset = ImageFolder(os.path.join(data_dir, 'x_train'), transform=transform)\nval_dataset = ImageFolder(os.path.join(data_dir, 'x_val'), transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n\n# DataLoader로부터 이미지와 라벨을 가져오는 방법\nfor batch_images, batch_labels in train_dataset:\n    # batch_labels는 각 이미지의 라벨을 나타냅니다.\n    print(batch_labels)\n    for label in batch_labels:\n        # 라벨의 원래 이름을 가져오는 방법은 데이터셋 객체의 클래스를 이용하는 것입니다.\n        original_label_name = train_loader.classes[label.item()]\n        print(\"Original Label Name:\", original_label_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import Resize, Normalize, Compose\nfrom transformers import ViTForImageClassification, ViTConfig, ViTImageProcessor\nfrom torchvision.transforms import ToTensor, Resize, Normalize, Compose\nfrom tqdm import tqdm  # tqdm import\nfrom torch import nn\nfrom sklearn.metrics import f1_score\n\n# Set up the dataset and dataloader\ndata_dir = '/kaggle/input/dataset/project (2)'\nbatch_size = 32\nnum_workers = 4\n\n# Define the transforms\ntransform = Compose([\n    Resize((224, 224)),\n    ToTensor(),  # PIL 이미지를 Tensor로 변환\n    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])\n\n# Load the dataset\ntrain_dataset = ImageFolder(os.path.join(data_dir, 'x_train'), transform=transform)\nval_dataset = ImageFolder(os.path.join(data_dir, 'x_val'), transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n\n# Load the pre-trained ViT model\nconfig = ViTConfig.from_pretrained('google/vit-base-patch16-224')\nconfig.num_labels = len(train_dataset.classes)  # Set the number of output classes\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', config=config,ignore_mismatched_sizes=True)\nmodel.classifier = nn.Linear(model.config.hidden_size, len(train_dataset.classes))\n\n# Fine-tune the model\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\nprint(device)\n\nlearning_rate = 2e-5\nnum_epochs = 10\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Import f1_score from sklearn.metrics\nfrom sklearn.metrics import f1_score\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    \n    # train_loader 반복 시 tqdm 적용\n    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)[0]\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        \n        # Training 중 각 iteration마다 macro F1 점수 계산 및 출력\n        preds = outputs.argmax(dim=1)\n#         macro_f1 = f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro')\n#         print(f\"Training iteration: Macro F1: {macro_f1:.4f}\")\n    \n    train_loss /= len(train_loader)\n    \n    model.eval()\n    val_loss = 0\n    y_true = []\n    y_pred = []\n    \n    # val_loader 반복 시 tqdm 적용\n    for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(images)[0]\n        loss = criterion(outputs, labels)\n        \n        val_loss += loss.item()\n        \n        preds = outputs.argmax(dim=1)\n        \n        # 각 iteration마다 macro F1 점수 계산 및 출력\n#         macro_f1 = f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro')\n#         print(f\"Validation iteration: Macro F1: {macro_f1:.4f}\")\n        \n        # 실제 레이블과 예측된 레이블을 리스트에 추가\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(preds.cpu().numpy())\n    \n    val_loss /= len(val_loader)\n    \n    # 전체 val_loader를 평가한 후 macro F1 점수 계산\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Macro F1: {macro_f1:.4f}')\n    \n    early_stopping.on_validation_epoch_end(epoch, {'val_loss': val_loss, 'val_macro_f1': macro_f1})\n    checkpoint_callback.on_validation_epoch_end(epoch, {'val_loss': val_loss, 'val_macro_f1': macro_f1})\n# Save the fine-tuned model\ntorch.save(model.state_dict(), 'fine_tuned_vit_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:47:04.865324Z","iopub.execute_input":"2024-04-18T05:47:04.866370Z","iopub.status.idle":"2024-04-18T05:55:45.740522Z","shell.execute_reply.started":"2024-04-18T05:47:04.866334Z","shell.execute_reply":"2024-04-18T05:55:45.739140Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-18 05:47:06.171337: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-18 05:47:06.171399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-18 05:47:06.172987: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([25]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([25, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"cuda\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10 - Training: 100%|██████████| 396/396 [03:48<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training iteration: Macro F1: 0.8626\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10 - Validation: 100%|██████████| 99/99 [00:21<00:00,  4.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation iteration: Macro F1: 0.0187\nEpoch [1/10], Train Loss: 0.7041, Train Macro F1: 0.8626, Val Loss: 0.1225, Val Macro F1: 0.0187\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10 - Training: 100%|██████████| 396/396 [03:48<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training iteration: Macro F1: 0.9895\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10 - Validation: 100%|██████████| 99/99 [00:21<00:00,  4.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation iteration: Macro F1: 0.0190\nEpoch [2/10], Train Loss: 0.0522, Train Macro F1: 0.9895, Val Loss: 0.0772, Val Macro F1: 0.0190\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10 - Training:   5%|▍         | 19/396 [00:11<03:57,  1.59it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     65\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 67\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Training 중 각 iteration마다 macro F1 점수 계산 및 출력\u001b[39;00m\n\u001b[1;32m     70\u001b[0m preds \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import Resize, Normalize, Compose, ToTensor\n\n# 1. 테스트 데이터셋 준비\ntest_dir = '/kaggle/input/dataset/test'\ntest_transform = Compose([\n    Resize((224, 224)),\n    ToTensor(),\n    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])\ntest_dataset = ImageFolder(test_dir, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# 2. 모델 로드 및 예측 수행\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', config=config)\nmodel.load_state_dict(torch.load('fine_tuned_vit_model.pth'))\nmodel.to(device)\nmodel.eval()\n\nall_preds = []\nfor images, _ in test_loader:\n    images = images.to(device)\n    outputs = model(images)[0]\n    preds = outputs.argmax(dim=1)\n    all_preds.extend(preds.cpu().numpy())\n\n# 3. 제출 파일 생성\ntest_filenames = [os.path.splitext(os.path.basename(path))[0] for path in test_dataset.imgs]\nsubmission = pd.DataFrame({'id': test_filenames, 'label': all_preds})\nsubmission.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batch_size = 64\n\n# # 데이터로더를 생성합니다.\n# train_dataloader = DataLoader(training_data, batch_size=batch_size)\n# test_dataloader = DataLoader(test_data, batch_size=batch_size)\n\n# for X, y in val_loader:\n#     print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n#     print(f\"Shape of y: {y.shape} {y.dtype}\")\n#     break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.transforms import Compose, Resize, ToTensor, Normalize\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\ntest_dir = '/kaggle/input/dataset/test'\ntest_transform = Compose([\n    Resize((224, 224)),\n    ToTensor(),\n    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])\n\nclass TestDataset(Dataset):\n    def __init__(self, file_paths, transform=None):\n        self.file_paths = file_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.file_paths[idx]\n        img = Image.open(img_path).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        return img, img_path\n\n# test_dir의 이미지 파일 리스트 생성\ntest_files = [os.path.join(test_dir, f) for f in os.listdir(test_dir)]\n\n# 레이블이 없는 테스트 데이터셋 클래스 인스턴스화\ntest_dataset = TestDataset(test_files, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:11:45.569586Z","iopub.execute_input":"2024-04-18T07:11:45.569954Z","iopub.status.idle":"2024-04-18T07:11:45.772049Z","shell.execute_reply.started":"2024-04-18T07:11:45.569925Z","shell.execute_reply":"2024-04-18T07:11:45.771266Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import Resize, Normalize, Compose, ToTensor\nfrom transformers import ViTForImageClassification, ViTConfig, ViTImageProcessor\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\n\n\n# 2. 모델 로드 및 예측 수행\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nconfig = ViTConfig.from_pretrained('google/vit-base-patch16-224')\nconfig.num_labels = 25 # 테스트 데이터셋의 클래스 수로 설정\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', config=config,ignore_mismatched_sizes=True)\nmodel.classifier = nn.Linear(model.config.hidden_size, 25)\nmodel.load_state_dict(torch.load('/kaggle/working/fine_tuned_vit_model.pth'))\nmodel.to(device)\nmodel.eval()\n\nall_preds = []\nall_paths = []\nfor images, names in tqdm(test_loader, '로딩'):\n    images = images.to(device)\n    outputs = model(images)[0]\n    preds = outputs.argmax(dim=1).tolist()\n    all_preds.extend(preds)\n     all_paths.extend(names)\n\n# 3. 제출 파일 생성\ntest_filenames = [os.path.splitext(os.path.basename(path))[0] for path in all_paths]\nsubmission = pd.DataFrame({'id': test_filenames, 'label': all_preds})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:15:12.205526Z","iopub.execute_input":"2024-04-18T07:15:12.206428Z","iopub.status.idle":"2024-04-18T07:15:58.039308Z","shell.execute_reply.started":"2024-04-18T07:15:12.206395Z","shell.execute_reply":"2024-04-18T07:15:58.038231Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([25]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([25, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n로딩: 100%|██████████| 425/425 [00:44<00:00,  9.62it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataLoader로부터 이미지 파일의 경로 추출\nall_paths = []\nfor batch_images, batch_paths in test_loader:\n    all_paths.extend(batch_paths)\n\n# all_paths에는 모든 이미지 파일의 경로가 저장됩니다.","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:18:04.801003Z","iopub.execute_input":"2024-04-18T07:18:04.801719Z","iopub.status.idle":"2024-04-18T07:18:12.056062Z","shell.execute_reply.started":"2024-04-18T07:18:04.801689Z","shell.execute_reply":"2024-04-18T07:18:12.054760Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"test_filenames = [os.path.splitext(os.path.basename(path))[0] for path in all_paths]","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:18:30.794668Z","iopub.execute_input":"2024-04-18T07:18:30.795605Z","iopub.status.idle":"2024-04-18T07:18:30.819716Z","shell.execute_reply.started":"2024-04-18T07:18:30.795571Z","shell.execute_reply":"2024-04-18T07:18:30.818794Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# 데이터셋 객체의 classes 속성을 사용하여 각 폴더의 이름을 가져옵니다.\nclass_names = train_dataset.classes","metadata":{"execution":{"iopub.status.busy":"2024-04-18T08:01:04.530094Z","iopub.execute_input":"2024-04-18T08:01:04.530556Z","iopub.status.idle":"2024-04-18T08:01:04.537216Z","shell.execute_reply.started":"2024-04-18T08:01:04.530524Z","shell.execute_reply":"2024-04-18T08:01:04.535568Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# 각 클래스 이름에 0부터 24까지의 정수 값을 할당하는 사전 생성\nclass_to_index = {class_name: index for index, class_name in enumerate(class_names)}\n\nclasses={}\n# 결과를 출력\nfor class_name, index in class_to_index.items():\n    classes[index]=class_name\n    print(f\"{index}: {class_name}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T08:05:55.770165Z","iopub.execute_input":"2024-04-18T08:05:55.770662Z","iopub.status.idle":"2024-04-18T08:05:55.779787Z","shell.execute_reply.started":"2024-04-18T08:05:55.770621Z","shell.execute_reply":"2024-04-18T08:05:55.778193Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"0: Asian Green Bee-Eater\n1: Brown-Headed Barbet\n2: Cattle Egret\n3: Common Kingfisher\n4: Common Myna\n5: Common Rosefinch\n6: Common Tailorbird\n7: Coppersmith Barbet\n8: Forest Wagtail\n9: Gray Wagtail\n10: Hoopoe\n11: House Crow\n12: Indian Grey Hornbill\n13: Indian Peacock\n14: Indian Pitta\n15: Indian Roller\n16: Jungle Babbler\n17: Northern Lapwing\n18: Red-Wattled Lapwing\n19: Ruddy Shelduck\n20: Rufous Treepie\n21: Sarus Crane\n22: White Wagtail\n23: White-Breasted Kingfisher\n24: White-Breasted Waterhen\n","output_type":"stream"}]},{"cell_type":"code","source":"classes","metadata":{"execution":{"iopub.status.busy":"2024-04-18T08:06:01.235771Z","iopub.execute_input":"2024-04-18T08:06:01.236628Z","iopub.status.idle":"2024-04-18T08:06:01.245586Z","shell.execute_reply.started":"2024-04-18T08:06:01.236592Z","shell.execute_reply":"2024-04-18T08:06:01.243992Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{0: 'Asian Green Bee-Eater',\n 1: 'Brown-Headed Barbet',\n 2: 'Cattle Egret',\n 3: 'Common Kingfisher',\n 4: 'Common Myna',\n 5: 'Common Rosefinch',\n 6: 'Common Tailorbird',\n 7: 'Coppersmith Barbet',\n 8: 'Forest Wagtail',\n 9: 'Gray Wagtail',\n 10: 'Hoopoe',\n 11: 'House Crow',\n 12: 'Indian Grey Hornbill',\n 13: 'Indian Peacock',\n 14: 'Indian Pitta',\n 15: 'Indian Roller',\n 16: 'Jungle Babbler',\n 17: 'Northern Lapwing',\n 18: 'Red-Wattled Lapwing',\n 19: 'Ruddy Shelduck',\n 20: 'Rufous Treepie',\n 21: 'Sarus Crane',\n 22: 'White Wagtail',\n 23: 'White-Breasted Kingfisher',\n 24: 'White-Breasted Waterhen'}"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nall_pred =pd.read_csv('/kaggle/working/submission.csv')\nall_pred.sort_values(by='id',inplace=True)\nall_pred['label']=all_pred['label'].map(classes)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T08:06:32.060818Z","iopub.execute_input":"2024-04-18T08:06:32.061269Z","iopub.status.idle":"2024-04-18T08:06:32.086825Z","shell.execute_reply.started":"2024-04-18T08:06:32.061231Z","shell.execute_reply":"2024-04-18T08:06:32.085639Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"all_pred","metadata":{"execution":{"iopub.status.busy":"2024-04-18T08:06:34.803424Z","iopub.execute_input":"2024-04-18T08:06:34.803850Z","iopub.status.idle":"2024-04-18T08:06:34.819113Z","shell.execute_reply.started":"2024-04-18T08:06:34.803819Z","shell.execute_reply":"2024-04-18T08:06:34.817832Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"              id                  label\n759   TEST_00000  Asian Green Bee-Eater\n850   TEST_00001  Asian Green Bee-Eater\n3621  TEST_00002         Jungle Babbler\n754   TEST_00003            Sarus Crane\n3748  TEST_00004       Northern Lapwing\n...          ...                    ...\n4953  TEST_06781      Common Kingfisher\n3202  TEST_06782           Gray Wagtail\n3726  TEST_06783            Sarus Crane\n2623  TEST_06784                 Hoopoe\n6418  TEST_06785     Coppersmith Barbet\n\n[6786 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>759</th>\n      <td>TEST_00000</td>\n      <td>Asian Green Bee-Eater</td>\n    </tr>\n    <tr>\n      <th>850</th>\n      <td>TEST_00001</td>\n      <td>Asian Green Bee-Eater</td>\n    </tr>\n    <tr>\n      <th>3621</th>\n      <td>TEST_00002</td>\n      <td>Jungle Babbler</td>\n    </tr>\n    <tr>\n      <th>754</th>\n      <td>TEST_00003</td>\n      <td>Sarus Crane</td>\n    </tr>\n    <tr>\n      <th>3748</th>\n      <td>TEST_00004</td>\n      <td>Northern Lapwing</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4953</th>\n      <td>TEST_06781</td>\n      <td>Common Kingfisher</td>\n    </tr>\n    <tr>\n      <th>3202</th>\n      <td>TEST_06782</td>\n      <td>Gray Wagtail</td>\n    </tr>\n    <tr>\n      <th>3726</th>\n      <td>TEST_06783</td>\n      <td>Sarus Crane</td>\n    </tr>\n    <tr>\n      <th>2623</th>\n      <td>TEST_06784</td>\n      <td>Hoopoe</td>\n    </tr>\n    <tr>\n      <th>6418</th>\n      <td>TEST_06785</td>\n      <td>Coppersmith Barbet</td>\n    </tr>\n  </tbody>\n</table>\n<p>6786 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"all_pred.to_csv('submission3.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T08:07:09.188560Z","iopub.execute_input":"2024-04-18T08:07:09.188953Z","iopub.status.idle":"2024-04-18T08:07:09.213673Z","shell.execute_reply.started":"2024-04-18T08:07:09.188924Z","shell.execute_reply":"2024-04-18T08:07:09.212380Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"len(all_preds)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T08:01:09.120694Z","iopub.execute_input":"2024-04-18T08:01:09.121463Z","iopub.status.idle":"2024-04-18T08:01:09.173130Z","shell.execute_reply.started":"2024-04-18T08:01:09.121416Z","shell.execute_reply":"2024-04-18T08:01:09.171275Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mall_preds\u001b[49m)\n","\u001b[0;31mNameError\u001b[0m: name 'all_preds' is not defined"],"ename":"NameError","evalue":"name 'all_preds' is not defined","output_type":"error"}]},{"cell_type":"code","source":"submission = pd.DataFrame({'id': test_filenames, 'label': all_preds})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:30:40.736192Z","iopub.execute_input":"2024-04-18T07:30:40.736930Z","iopub.status.idle":"2024-04-18T07:30:40.776776Z","shell.execute_reply.started":"2024-04-18T07:30:40.736898Z","shell.execute_reply":"2024-04-18T07:30:40.775596Z"},"trusted":true},"execution_count":26,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: test_filenames, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mclass_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m]\u001b[49m})\n\u001b[1;32m      2\u001b[0m submission\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"],"ename":"TypeError","evalue":"unhashable type: 'list'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:29:30.484333Z","iopub.execute_input":"2024-04-18T07:29:30.484700Z","iopub.status.idle":"2024-04-18T07:29:30.505545Z","shell.execute_reply.started":"2024-04-18T07:29:30.484671Z","shell.execute_reply":"2024-04-18T07:29:30.504673Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"             label\ncount  6786.000000\nmean     11.818892\nstd       7.234369\nmin       0.000000\n25%       6.000000\n50%      12.000000\n75%      18.000000\nmax      24.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>6786.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>11.818892</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7.234369</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>6.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>12.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>18.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>24.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:27:11.324383Z","iopub.execute_input":"2024-04-18T07:27:11.324747Z","iopub.status.idle":"2024-04-18T07:27:11.343878Z","shell.execute_reply.started":"2024-04-18T07:27:11.324711Z","shell.execute_reply":"2024-04-18T07:27:11.342952Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"              id  label\n0     TEST_04038     16\n1     TEST_04342     18\n2     TEST_00733     13\n3     TEST_03764      3\n4     TEST_03144     17\n...          ...    ...\n6781  TEST_00659      5\n6782  TEST_06036      7\n6783  TEST_06657     20\n6784  TEST_04091     16\n6785  TEST_03967     18\n\n[6786 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TEST_04038</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TEST_04342</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TEST_00733</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TEST_03764</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TEST_03144</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6781</th>\n      <td>TEST_00659</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6782</th>\n      <td>TEST_06036</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>6783</th>\n      <td>TEST_06657</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>6784</th>\n      <td>TEST_04091</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>6785</th>\n      <td>TEST_03967</td>\n      <td>18</td>\n    </tr>\n  </tbody>\n</table>\n<p>6786 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_filenames","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_filenames = [os.path.splitext(os.path.basename(path))[0] for path in test_dataset.imgs]\nsubmission = pd.DataFrame({'id': test_filenames, 'label': all_preds})\nsubmission.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import Resize, Normalize, Compose, ToTensor\n\n# 1. 테스트 데이터셋 준비\ntest_dir = '/kaggle/input/dataset'\ntest_transform = Compose([\n    Resize((224, 224)),\n    ToTensor(),\n    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])\ntest_dataset = ImageFolder(test_dir, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# 2. 모델 로드 및 예측 수행\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nconfig = ViTConfig.from_pretrained('google/vit-base-patch16-224')\nconfig.num_labels = len(test_dataset.classes)  # 테스트 데이터셋의 클래스 수로 설정\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', config=config)\nmodel.classifier = nn.Linear(model.config.hidden_size, len(test_dataset.classes))\nmodel.load_state_dict(torch.load('fine_tuned_vit_model.pth'))\nmodel.to(device)\nmodel.eval()\n\n\nall_preds = []\nfor images, _ in test_loader:\n    images = images.to(device)\n    outputs = model(images)[0]\n    preds = outputs.argmax(dim=1)\n    all_preds.extend([test_dataset.classes[p] for p in preds.cpu().numpy()])\n\n# 3. 제출 파일 생성\ntest_filenames = [os.path.splitext(os.path.basename(path))[0] for path in test_dataset.imgs]\nsubmission = pd.DataFrame({'id': test_filenames, 'label': all_preds})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T06:47:46.828542Z","iopub.execute_input":"2024-04-18T06:47:46.828989Z","iopub.status.idle":"2024-04-18T06:47:57.225315Z","shell.execute_reply.started":"2024-04-18T06:47:46.828956Z","shell.execute_reply":"2024-04-18T06:47:57.223962Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([25]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([25, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(images)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m     preds \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     all_preds\u001b[38;5;241m.\u001b[39mextend([test_dataset\u001b[38;5;241m.\u001b[39mclasses[p] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 3. 제출 파일 생성\u001b[39;00m\n\u001b[1;32m     33\u001b[0m test_filenames \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(path))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m test_dataset\u001b[38;5;241m.\u001b[39mimgs]\n","Cell \u001b[0;32mIn[8], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(images)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m     preds \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     all_preds\u001b[38;5;241m.\u001b[39mextend([\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 3. 제출 파일 생성\u001b[39;00m\n\u001b[1;32m     33\u001b[0m test_filenames \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(path))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m test_dataset\u001b[38;5;241m.\u001b[39mimgs]\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}]},{"cell_type":"code","source":"print(len(test_dataset.classes))\nprint(config.num_labels)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T06:50:27.274869Z","iopub.execute_input":"2024-04-18T06:50:27.275244Z","iopub.status.idle":"2024-04-18T06:50:27.280677Z","shell.execute_reply.started":"2024-04-18T06:50:27.275215Z","shell.execute_reply":"2024-04-18T06:50:27.279715Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"2\n25\n","output_type":"stream"}]}]}