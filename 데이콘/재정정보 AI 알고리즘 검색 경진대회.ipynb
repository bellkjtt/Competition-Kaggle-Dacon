{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d60267",
   "metadata": {},
   "source": [
    "## 재정정보 AI 검색 알고리즘 경진대회 코드\n",
    "대회 주소 : https://dacon.io/competitions/official/236295/overview/description\n",
    "### 시스템 환경\n",
    "OS : Docker - pytorch/pytorch:2.3.1-cuda12.1-cudnn8-runtime <br>\n",
    "CPU : Xeon® E5-2698 v4 <br> \n",
    "RAM : 97GB <br>\n",
    "GPU : Q RTX 8000(VRAM 45GB) x 2 <br>\n",
    "<br>\n",
    "사용 모델 : rtzr/ko-gemma-2-9b-it, intfloat/multilingual-e5-large, Dongjin-kr/ko-reranker  <br>\n",
    "\n",
    "### 제약사항 : 유료 API 전부 사용 불가능\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4da0d611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.44.0 in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (4.44.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from transformers==4.44.0) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from transformers==4.44.0) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from transformers==4.44.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from transformers==4.44.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from transformers==4.44.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from transformers==4.44.0) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from transformers==4.44.0) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from transformers==4.44.0) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from transformers==4.44.0) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from transformers==4.44.0) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.0) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from tqdm>=4.27->transformers==4.44.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from requests->transformers==4.44.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from requests->transformers==4.44.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from requests->transformers==4.44.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administer\\anaconda3\\envs\\aa\\lib\\site-packages (from requests->transformers==4.44.0) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.44.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e59966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install accelerate\n",
    "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "\n",
    "!pip install datasets\n",
    "!pip install langchain\n",
    "!pip install langchain_community\n",
    "!pip install PyMuPDF\n",
    "!pip install sentence-transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install kiwipiepy rank_bm25 openai tiktoken konlpy pdfplumber\n",
    "!pip install -U langchain-experimental # 최신 버전이 필요합니다 (멀티 모달을 위해)\n",
    "!pip install langchain-teddynote pymupdf pymupdf4llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74f16f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo apt update -y\n",
    "!sudo apt install openjdk-11-jdk -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9662f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# # API 키 정보 로드\n",
    "# load_dotenv()\n",
    "\n",
    "import json\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from transformers import set_seed\n",
    "\n",
    "# 시드 설정\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pymupdf\n",
    "import pymupdf4llm\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    AwqConfig\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Langchain 관련\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS, Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_teddynote.retrievers import KiwiBM25Retriever,OktBM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever, MultiQueryRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "\n",
    "from langchain.document_loaders import PDFPlumberLoader, PyMuPDFLoader, PyPDFLoader, UnstructuredPDFLoader\n",
    "# from peft import PeftModel\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from kiwipiepy import Kiwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53825777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opt:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # 선택된 모델\n",
    "        self.base_directory = \"\"\n",
    "\n",
    "        self.llm_model = os.path.join(self.base_directory, \"rtzr\")\n",
    "        # self.llm_model_config = self.model_configs[self.llm_model]\n",
    "        self.llm_peft = False\n",
    "        self.llm_peft_checkpoint = \"rtzr/ko-gemma-2-9b-it\"\n",
    "        \n",
    "        self.embed_models = [\"intfloat/multilingual-e5-base\", \"jhgan/ko-sbert-nli\", \"intfloat/multilingual-e5-large\"]\n",
    "        self.embed_model = self.embed_models[2]\n",
    "        \n",
    "        self.pdf_loader= \"pymupdf-remaster\"\n",
    "        \n",
    "        self.train_csv_path = os.path.join(self.base_directory, \"train.csv\")\n",
    "        self.test_csv_path = os.path.join(self.base_directory, \"test.csv\")\n",
    "        self.chunk_size = 512\n",
    "        self.chunk_overlap = 32\n",
    "        \n",
    "        self.ensemble = True\n",
    "        self.bm25_w = 0.3\n",
    "        self.faiss_w = 0.7\n",
    "        \n",
    "        self.is_submit = True\n",
    "        \n",
    "        self.eval_sum_mode = False\n",
    "        \n",
    "        self.output_dir = \"test_results\"\n",
    "        self.output_csv_file = f\"{self.llm_model}_{self.embed_model.split('/')[1]}_pdf{self.pdf_loader}_chks{self.chunk_size}_chkovp{self.chunk_overlap}_bm25{self.bm25_w}_faiss{self.faiss_w}_mix_submission.csv\"\n",
    "        \n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.__dict__)\n",
    "    \n",
    "        \n",
    "args=Opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364f0489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llm_pipeline():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.llm_model)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "    base_directory = \"\"\n",
    "    llm_model = os.path.join(base_directory, \"rtzr\")\n",
    "    print(tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "\n",
    "    max_memory_mapping = {0: \"14GB\", 1: \"45GB\"}\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_model,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        max_memory=max_memory_mapping\n",
    "        # attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    \n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=adapter_model if args.llm_peft else model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.7,\n",
    "        top_k=50,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "120b6362",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43msetup_llm_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m, in \u001b[0;36msetup_llm_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_llm_pipeline\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39muse_default_system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     base_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:897\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    894\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    895\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    896\u001b[0m         )\n\u001b[1;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    899\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2271\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2269\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2272\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2273\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2274\u001b[0m     init_configuration,\n\u001b[0;32m   2275\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2276\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2277\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2278\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2279\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2280\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2281\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2282\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2283\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2505\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2503\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2504\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2505\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2506\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2508\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2509\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2510\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\transformers\\models\\gemma\\tokenization_gemma_fast.py:103\u001b[0m, in \u001b[0;36mGemmaTokenizerFast.__init__\u001b[1;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, pad_token, add_bos_token, add_eos_token, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     92\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    102\u001b[0m ):\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    104\u001b[0m         vocab_file\u001b[38;5;241m=\u001b[39mvocab_file,\n\u001b[0;32m    105\u001b[0m         tokenizer_file\u001b[38;5;241m=\u001b[39mtokenizer_file,\n\u001b[0;32m    106\u001b[0m         clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m    107\u001b[0m         unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[0;32m    108\u001b[0m         bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[0;32m    109\u001b[0m         eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[0;32m    110\u001b[0m         pad_token\u001b[38;5;241m=\u001b[39mpad_token,\n\u001b[0;32m    111\u001b[0m         add_bos_token\u001b[38;5;241m=\u001b[39madd_bos_token,\n\u001b[0;32m    112\u001b[0m         add_eos_token\u001b[38;5;241m=\u001b[39madd_eos_token,\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    114\u001b[0m     )\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_bos_token \u001b[38;5;241m=\u001b[39m add_bos_token\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:144\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m     )\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m fast_tokenizer\n\u001b[1;32m--> 144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mslow_tokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m:\n\u001b[0;32m    145\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(slow_tokenizer\u001b[38;5;241m.\u001b[39minit_kwargs)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm = setup_llm_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "706d460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # 연속된 공백을 하나의 공백으로 대체\n",
    "    pattern = r'\\x07'\n",
    "    pattern2 = r'\\x03'\n",
    "    # 줄 끝의 하이픈으로 끊긴 단어 연결\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "    # 줄바꿈 문자를 공백으로 대체 (옵션)\n",
    "    text = text.replace('\u0007', '')\n",
    "    text = text.replace('□', '')\n",
    "    text = text.replace('ㅇ', '')\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    cleaned_text = re.sub(pattern2, '', cleaned_text)\n",
    "    text_compact = re.sub(r'\\s+', ' ', cleaned_text).replace(' ', ' ')\n",
    "    \n",
    "    return text_compact.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f11319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_path(path):\n",
    "    return unicodedata.normalize('NFC', path)\n",
    "    \n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "def process_pdf(file_path,title):\n",
    "    md_text = pymupdf4llm.to_markdown(file_path)\n",
    "    \n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "\n",
    "    md_header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "    md_chunks = md_header_splitter.split_text(md_text)\n",
    "    dd=[]\n",
    "    kiwi=Kiwi()\n",
    "    for i in md_chunks:\n",
    "        sp = kiwi.split_into_sents(i.page_content)\n",
    "        for j in sp:\n",
    "            print(clean_text(j.text))\n",
    "            dd.append(Document(page_content=clean_text(j.text)))\n",
    "\n",
    "    return dd\n",
    "\n",
    "\n",
    "\n",
    "def create_vector_db(chunks, model_path,cnt, method='faiss'):\n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    \n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04a0b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs_from_dataframe(df, base_directory):\n",
    "    \"\"\"딕셔너리에 pdf명을 키로해서 DB, retriever 저장\"\"\"\n",
    "    pdf_databases = {}\n",
    "    unique_paths = df['Source_path'].unique()\n",
    "    cnt=0\n",
    "    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
    "        # 경로 정규화 및 절대 경로 생성\n",
    "        normalized_path = normalize_path(path)\n",
    "        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n",
    "        \n",
    "        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "        print(f\"Processing {pdf_title}...\")\n",
    "        \n",
    "        # PDF 처리 및 벡터 DB 생성\n",
    "        chunks = process_pdf(full_path,pdf_title)\n",
    "\n",
    "        db = create_vector_db(chunks, model_path=args.embed_model,cnt=cnt)\n",
    "            \n",
    "        Okt_bm25_retriever = OktBM25Retriever.from_documents(chunks)\n",
    "        Okt_bm25_retriever.k = 5\n",
    "        faiss_retriever = db.as_retriever(search_kwargs={'k': 5},)\n",
    " \n",
    "        retriever = EnsembleRetriever(\n",
    "            retrievers=[Okt_bm25_retriever, faiss_retriever],\n",
    "            weights=[args.bm25_w, args.faiss_w],\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={'k': 5},\n",
    "        )\n",
    "\n",
    "        # 결과 저장\n",
    "        pdf_databases[pdf_title] = {\n",
    "            'db': db,\n",
    "            'retriever': retriever\n",
    "        }\n",
    "    cnt+=1\n",
    "        \n",
    "    return pdf_databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "270507fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 중소벤처기업부_혁신창업사업화자금(융자)...\n",
      "## 사 업 명 혁신창업사업화자금(융자) (5152-301)\n",
      "## 1. 사업 코드 정보 |구분|기금|소관|실국(기관)|계정|분야|부문| |---|---|---|---|---|---|---| |코드|중소벤처기업 창업 및 진흥기금|중소벤처기업부|중소기업정책실||110|118| |명칭|||||산업·중소기업 및 에너지|창업 및 벤처| |구분|프로그램|단위사업|세부사업| |---|---|---|---| |코드|5100|5152|301| |명칭|창업환경조성|창업기업지원융자(기금)|혁신창업사업화자금(융자)|\n",
      "## 구분 기금 소관 실국(기관) 계정 분야 부문 코드 110 118 중소벤처기업 중소벤처기업부 중소기업정책실 산업·중소기업\n",
      "## 명칭 창업 및 진흥기금 창업 및 벤처 및 에너지\n",
      "## 구분 프로그램 단위사업 세부사업 코드 5100 5152 301 명칭 창업환경조성 창업기업지원융자(기금) 혁신창업사업화자금(융자) 2. 사업 지원 형태 및 지원율 |직접|출자|출연|보조|융자|국고보조율(%)|융자율 (%)| |---|---|---|---|---|---|---| ||||○|○|||\n",
      "## 직접 출자 출연 보조 융자 국고보조율(%) 융자율 (%) ○ ○ 3. 지출계획 총괄표 (단위: 백만원, %) |사업명|2022년 결산|2023년|Col4|2024년|증감 (B-A) (B-A)/A|Col7| |---|---|---|---|---|---|---| |||당초(A)|수정|확정(B)||(B-A)/A| |혁신창업사업화자 금(융자)|2,300,000|2,230,000|2,330,000|2,007,800|△222,200|△9.96| 2023년 2024년 2022년 증감 사업명 결산 당초(A) 수정 확정(B) (B-A) (B-A)/A 혁신창업사업화자 2,300,000 2,230,000 2,330,000 2,007,800 △222,200 △9.96 금(융자)\n",
      "## 4. 사업목적 (창업기반지원) 기술력과 사업성이 우수하고 미래 성장가능성이 높은 중소벤처기업의 창업을 활성화하고 고용창출 도모 (개발기술사업화) 중소기업이 보유한 우수 기술의 사장을 방지하고 개발기술의 제품화·사업화를 촉진하여 기술기반 중소기업을 육성\n",
      "## 5. 사업근거 및 추진경위 법령상 근거 : 중소기업진흥에 관한 법률 제66조, 제67조, 제74조 중소기업창업지원법 제35조 ----- 추진경위 - 1998. 4 외환위기 이후 창업 활성화 및 고용 창출을 위하여 지원 시작 - 2005. 2 업체별 연간 대출한도를 20억원으로 확대(기존 10억원) - 2006. 1 지원대상을 업력 5년미만까지 확대(기존 3년미만) - 2008. 1 부채비율 예외적용 기업을 업력 5년미만까지 확대(기존 3년 미만) - 2009. 1 지원대상을 업력 7년 미만까지 확대(기존 5년 미만) - 2010. 3 재창업자금(실패 경영인에 대한 재기지원) 신규 지원 - 2012. 1 지원대상을 업력 5년 미만으로 축소(기존 7년 미만) 청년전용창업자금(만 39세 이하 청년창업자 대상) 신규 지원 - 2014. 2 경제혁신 3개년 계획에 반영(청년전용창업자금, 재창업자금) - 2014. 1 지원대상 업력기준을 7년 미만으로 확대 - 2015. 1 재창업자금을 재도약지원자금(융자)의 내역사업으로 이관 - 2019. 1 청년전용창업자금을 혁신창업지원자금에 통합 운영 개발기술사업화자금을 내역사업으로 통합 및 일자리창출촉진자금 신규 지원 - 2020. 1 미래기술육성자금, 고성장촉진자금 신규 지원 - 2022. 1 미래기술육성자금 및 고성장촉진자금 사업 폐지 - 2023. 1 창업기반지원과 신청 대상이 중복인 일자리창출촉진자금을 폐지 - 2023. 1 성장공유형 대출방식 추가 (혁신창업사업화자금)\n",
      "# < 국정과제 > < 국정목표 2. 민간이 끌고 정부가 미는 역동적 경제 > 6. 중소·벤처기업이 경제의중심에 서는 나라 ◾(31번) 중소기업 정책을 민간주도 혁신성장의 관점에서 재설계 (중기부) - (금융·수출·판로 패키지) 중소기업 정책자금 확대, 수출 물류비 지원 강화 및 중소기업 전용 물류센터 구축, 공공분야의 혁신제품 구매 자율성 강화 ◾(32번) 예비창업부터 글로벌 유니콘까지 완결형 벤처 생태계 구현 - 중소기업을 위한 금융지원 강화\n",
      "## 6. 주요내용 총사업비 : 해당없음 사업기간 : ’98년~ 계속 사업규모 : ‘24년 2조 78억원 사업시행방법 : 융자, 보조 ----- 사업시행주체 : 중소벤처기업진흥공단 사업 수혜자 : 중소기업 보조, 융자, 출연, 출자 등의 경우 보조·융자 등 지원 비율 및 법적근거 |내역사업명|구분|피보조․피출연 등 기관명|지원 금액 (2024 계획안)|지원 비율(%)|보조율_법적_근거, (해당 조항)| |---|---|---|---|---|---| |창업기반지원|융자|중소기업|1조 7,958억원|정책자금기준금리△0.3%p|중소기업진흥에 관한 법률 제66조, 제67조, 제74조| |개발기술사업화|융자 +보조|중소기업|2,120억원|(융자)정책자금기준금리 (이차보전) 최대 5.5% 이내|중소기업진흥에 관한 법률 제66조, 제67조, 제74조|\n",
      "## 7. 사업 집행절차 사업계획수립/공고 서류 및 현장실사 ⇨ [사전상담 및 신청][‧][접수] ⇨ [‧] (중기부, 중진공) (중소기업 → 중진공) (중진공 → 중소기업) 사업계획수립/공고 (중기부, 중진공) 융자 실행 지원결정통보 평가 및 승인 ⇦ (중진공, 은행 → 중소기업) [⇦] (중진공 → 중소기업) (중진공 → 중소기업) 융자 실행 (중진공, 은행 → 중소기업) -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Administer\\.cache\\huggingface\\hub\\models--intfloat--multilingual-e5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Processing PDFs:  11%|█         | 1/9 [02:16<18:08, 136.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 보건복지부_부모급여(영아수당) 지원...\n",
      "# 사 업 명 (67) 부모급여(영아수당) 지원 (3143-300)\n",
      "# 1. 사업 코드 정보 |구분|회계|소관|실국(기관)|계정|분야|부문| |---|---|---|---|---|---|---| |코드|11 일반회계|23 보건복지부|인구정책실 보육정책관||080|08A| |명칭|||||사회복지|아동·보육| |구분 코드 명칭|회계 소관 11 일반회계 23 보건복지부|실국(기관) 계정 인구정책실 보육정책관|분야 부문 080 08A 사회복지 아동·보육| |---|---|---|---| |구분|프로그램|단위사업|세부사업| |코드|3100|3143|300| |명칭|보육지원강화|부모급여(영아수당) 지원|부모급여(영아수당) 지원| 2. 사업 지원 형태 및 지원율 |직접|출자|출연|보조|융자|국고보조율(%)|융자율 (%)| |---|---|---|---|---|---|---| ||||○||서울 25~45% 지방 55~75%|| 3. 예산 총괄표 |Col1|Col2|Col3|Col4|Col5|(단위: 백만원, %)|Col7| |---|---|---|---|---|---|---| |사업명|2022년 결산|2023년 예산 본예산(A)|2024년||증감 (B-A) (B-A)/A|| ||||정부안|확정(B)||(B-A)/A| |부모급여(영아수당) 지원|341,751|1,621,454|2,888,694|2,888,694|1,267,240|78.2| 2022년 2023년 예산 2024년 증감 사업명 결산 본예산(A) (B-A) 정부안 확정(B) (B-A)/A 부모급여(영아수당) 지원 341,751 1,621,454 2,888,694 2,888,694 1,267,240 78.2 4. 사업목적·내용 (단위: 백만원, %) (부모급여 지원) 출산 및 양육으로 손실되는 소득을 보전하고, 주 양육자의 직접돌봄이 중요한 아동발달의 특성에 따라 영아기 돌봄을 두텁게 지원하기 위해 부모급여 지급 - 영아수당을부모급여로변경‧확대 ‧ -----\n",
      "# 5. 사업근거 및 추진경위 ① 법령상 근거 및 조항 적시 : 아동수당법 제4조제5항 ◈아동수당법제4조(’23.9.14. 시행) 제4조(아동수당의지급대상및지급액) ①아동수당은8세미만의아동에게매월10만원을지급한다.\n",
      "⑤제1항에도불구하고만2세미만아동에게매월50만원이상으로서대통령령으로정 하는금액을추가로지급한다.\n",
      "부칙제6조(아동수당추가지급금액에관한특례) 제4조제5항의개정규정에도불구하고20 23년12월31일까지는30만원이상의금액중에서보건복지부장관이별도로정하여 고시하는금액을지급한다. ②\n",
      "추진경위 제 ‧ ｣ 5 - 예비타당성조사 통과(’21.8.), 근거법 마련(’21.12.) - 영아수당 지원사업 시행(’22.1~) - ‘부모급여 도입’ 대통령 공약사항 및 국정과제[*]에 포함(’22.5.) - (국정과제46-1) ’24년부터0～11개월아동에월100만원부모급여지급으로가정양육지원 및부모의경제적부담을획기적으로완화(’23년70만원, ’24년100만원으로단계적확대) (공약내용) 0개월～12개월아이키우는부모에게월100만원지원 - 부모급여 지원사업 시행(’23.1~) - 부모급여 지급근거를 규정한 ｢아동수당법｣ 개정(’23.5) ｢ ｣\n",
      "# 6. 주요내용 ① 사업규모 - 총사업비(해당되는 경우에만 기재) : 해당없음 -\n",
      "사업기간 : ’22.1~, 단년도 계속 사업 - 기타: (’22년 본예산 기준) 연간 1,817,446건 지원 예정(월평균 151천명) (’22년 추경 기준) 연간 1,698,339건 지원 예정(월평균 142천명) (’23년 본예산 기준) 연간 3,871,400건 지원 예정(월평균 323천명) (’24년 본예산 기준) 연간 5,658,766건 지원 예정(월평균 472천명) ----- ② 사업추진체계 - 사업시행방법 : 지자체 보조 - 사업시행주체 : 보건복지부, 시‧도 및 시‧군‧구 ‧ ‧군 ‧구 - 사업 수혜자 : 만 0~1세 아동 - 보조, 융자, 출연, 출자 등의 경우 보조․융자 등 지원 비율 및 법적근거 |내역사업명|구분|피보조․ 피출연 등 기관명|지원 금액 (2024예산)|지원 비율(%)|보조율_법적_근거, (해당 조항)| |---|---|---|---|---|---| |부모급여 (영아수당) 지원|보조|지자체 보조|2,888,694 백만원|서울 25~45 지방 55~75|보조금 관리에 관한 법률 시행령 제4조 제1항 (별표 1)|\n",
      "# 7. 사업 집행절차 보건복지부 지원기준 마련 및 국고보조금 교부 ↓ 시·도 국고보조금에 지방비(시·도비)를 가산하여 시·군·구에 보조금 교부 ↓ 지원대상자 선정, 급여액 확정 및 지급 및 정산 시·군·구 ※ e-지로시스템(금융결제원) 또는 각 시‧군‧구 금고 시스템을 통해 지급 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  11%|█         | 1/9 [03:26<27:30, 206.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m test_pdf_databases \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mis_submit:\n\u001b[1;32m----> 7\u001b[0m     test_pdf_databases \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_pdfs_from_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     train_pdf_databases \u001b[38;5;241m=\u001b[39m process_pdfs_from_dataframe(train_df, base_directory)\n",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m, in \u001b[0;36mprocess_pdfs_from_dataframe\u001b[1;34m(df, base_directory)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m# PDF 처리 및 벡터 DB 생성\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m process_pdf(full_path,pdf_title)\n\u001b[1;32m---> 17\u001b[0m         db \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vector_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcnt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcnt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#         # 부모 문서를 생성하는 데 사용되는 텍스트 분할기입니다.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#         parent_splitter = RecursiveCharacterTextSplitter(chunk_size=900)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#         # 자식 문서를 생성하는 데 사용되는 텍스트 분할기입니다.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#         retriever.add_documents(chunks)  # 문서를 retriever에 추가합니다.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#         print(len(list(store.yield_keys())))\u001b[39;00m\n\u001b[0;32m     41\u001b[0m         Okt_bm25_retriever \u001b[38;5;241m=\u001b[39m OktBM25Retriever\u001b[38;5;241m.\u001b[39mfrom_documents(chunks)\n",
      "Cell \u001b[1;32mIn[9], line 45\u001b[0m, in \u001b[0;36mcreate_vector_db\u001b[1;34m(chunks, model_path, cnt, method)\u001b[0m\n\u001b[0;32m     39\u001b[0m     encode_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalize_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[0;32m     40\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(\n\u001b[0;32m     41\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mmodel_path,\n\u001b[0;32m     42\u001b[0m         model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[0;32m     43\u001b[0m         encode_kwargs\u001b[38;5;241m=\u001b[39mencode_kwargs\n\u001b[0;32m     44\u001b[0m     )\n\u001b[1;32m---> 45\u001b[0m     db \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m#     vectorstore = Chroma(\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m#             collection_name=f\"split_parents-{cnt}\", embedding_function=embeddings\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#         )\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m db\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:833\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    831\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    832\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 833\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1039\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1020\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1039\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m   1041\u001b[0m         texts,\n\u001b[0;32m   1042\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1047\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:103\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    101\u001b[0m     sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39mstop_multi_process_pool(pool)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mencode(\n\u001b[0;32m    104\u001b[0m         texts, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_progress, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_kwargs\n\u001b[0;32m    105\u001b[0m     )\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:517\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    514\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 517\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    519\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:118\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m    116\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 118\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    119\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    121\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:834\u001b[0m, in \u001b[0;36mXLMRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    825\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    827\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    828\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    829\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    832\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    833\u001b[0m )\n\u001b[1;32m--> 834\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    847\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:522\u001b[0m, in \u001b[0;36mXLMRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    511\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    512\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    513\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         output_attentions,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 522\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:411\u001b[0m, in \u001b[0;36mXLMRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    401\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    408\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 411\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:338\u001b[0m, in \u001b[0;36mXLMRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    330\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    337\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 338\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administer\\anaconda3\\envs\\aa\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:267\u001b[0m, in \u001b[0;36mXLMRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m attention_probs \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[1;32m--> 267\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    270\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_directory = args.base_directory  # Your Base Directory\n",
    "train_df = pd.read_csv(args.train_csv_path)\n",
    "test_df = pd.read_csv(args.test_csv_path)\n",
    "train_pdf_databases = None\n",
    "test_pdf_databases = None\n",
    "if args.is_submit:\n",
    "    test_pdf_databases = process_pdfs_from_dataframe(test_df, base_directory)\n",
    "else:\n",
    "    train_pdf_databases = process_pdfs_from_dataframe(train_df, base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2edc9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df if args.is_submit else train_df\n",
    "pdf_databases = test_pdf_databases if args.is_submit else train_pdf_databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97cc0075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    \"\"\"유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "717371bc-d2b2-402f-850d-207d1841368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import format_document\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58874867-694d-48cb-bd60-e6911d8504f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 문서 프롬프트를 생성합니다. (source, metadata 등을 추가할 수 있습니다)\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(\n",
    "    template=\"{page_content}]\"\n",
    ")\n",
    "\n",
    "def combine_documents(\n",
    "    docs,  # 문서 목록\n",
    "    # 문서 프롬프트 (기본값: DEFAULT_DOCUMENT_PROMPT)\n",
    "    document_prompt=DEFAULT_DOCUMENT_PROMPT,\n",
    "    document_separator=\"\\n\",  # 문서 구분자 (기본값: 두 개의 줄바꿈)\n",
    "):\n",
    "    # context 에 입력으로 넣기 위한 문서 병합\n",
    "    doc_strings = [\n",
    "        f\"[{i}] {format_document(doc, document_prompt)}\" for i, doc in enumerate(docs)\n",
    "    ]  # 각 문서를 주어진 프롬프트로 포맷팅하여 문자열 목록 생성\n",
    "    return document_separator.join(\n",
    "        doc_strings\n",
    "    )  # 포맷팅된 문서 문자열을 구분자로 연결하여 반환\n",
    "\n",
    "\n",
    "def reorder_documents(docs):\n",
    "    # 재정렬\n",
    "    # reordering = LongContextReorder()\n",
    "    # reordered_docs = reordering.transform_documents(docs)\n",
    "    combined = combine_documents(docs, document_separator=\"\\n\")\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c6d0d60-dfb9-43ec-aa27-cdec447b06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name=\"Dongjin-kr/ko-reranker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d1a29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:   0%|          | 0/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 2022년 혁신창업사업화자금(융자)의 예산은 얼마인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:   1%|          | 1/98 [00:12<20:55, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원 입니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 중소벤처기업부의 혁신창업사업화자금(융자) 사업목적은 무엇인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:   2%|▏         | 2/98 [00:42<36:16, 22.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 중소벤처기업부의 혁신창업사업화자금(융자) 사업목적은 기술력과 사업성이 우수하고 미래 성장가능성이 높은 중소벤처기업의 창업을 활성화하고 고용창출 도모하며, 중소기업이 보유한 우수 기술의 사장을 방지하고 개발기술의 제품화·사업화를 촉진하여 기술기반 중소기업을 육성하는 것입니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 중소벤처기업부의 혁신창업사업화자금(융자) 사업근거는 어떤 법률에 근거하고 있나요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:   3%|▎         | 3/98 [00:56<29:45, 18.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 중소기업진흥에 관한 법률 제66조, 제67조, 제74조, 중소기업창업지원법 제35조 에 근거하고 있습니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 2010년에 신규 지원된 혁신창업사업화자금은 무엇인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:   4%|▍         | 4/98 [01:10<26:12, 16.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 2010년에 신규 지원된 혁신창업사업화자금은 재창업자금(실패 경영인에 대한 재기지원) 입니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 혁신창업사업화자금 중 2020년에 신규 지원된 자금은 무엇인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:   5%|▌         | 5/98 [01:20<22:37, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 2020년에 신규 지원된 자금은 미래기술육성자금, 고성장촉진자금 입니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 재창업자금이 재도약지원자금으로 이관된 연도는 언제인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:   6%|▌         | 6/98 [01:31<20:13, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 재창업자금이 재도약지원자금으로 이관된 연도는 2015년입니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 창업기반지원과 신청 대상이 중복인 자금이 어떤 것이며, 이 자금이 폐지된 연도는 언제인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:   7%|▋         | 7/98 [01:46<21:06, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 창업기반지원과 신청 대상이 중복되는 자금은 '일자리창출촉진자금'이며, 이 자금은 2023년 1월에 폐지되었습니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 혁신창업사업화자금(융자) 사업을 시행하는 주체는 누구인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:   8%|▊         | 8/98 [01:53<17:16, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 중소벤처기업진흥공단 \n",
      "\n",
      "\n",
      "Question: 혁신창업사업화자금(융자) 사업 집행절차는 어떻게 되나요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:   9%|▉         | 9/98 [02:47<37:03, 24.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 혁신창업사업화자금(융자) 사업의 집행 절차는 다음과 같습니다.\n",
      "\n",
      "    1. 사업계획 수립 및 공고: 중소벤처기업부와 중소기업은 사업계획을 수립하고 공고를 발표합니다.\n",
      "    2. 사전 상담 및 신청: 중소기업은 사전 상담을 통해 필요한 정보를 얻고 신청서를 제출합니다.\n",
      "    3. 접수 및 현장실사: 중소기업은 신청서를 중진공에 접수하고, 중진공은 현장실사를 진행합니다.\n",
      "    4. 평가 및 승인: 중진공은 평가 결과를 바탕으로 사업을 승인합니다.\n",
      "    5. 사업 실행 지원 결정 통보: 중진공은 승인된 사업에 대해 지원 결정을 통보합니다.\n",
      "    6. 융자 실행: 중진공 또는 은행은 중소기업에게 융자를 실행합니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 부모급여 지원 사업의 목적은 무엇인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  10%|█         | 10/98 [03:10<35:41, 24.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 부모급여 지원 사업의 목적은 출산 및 양육으로 손실되는 소득을 보전하고, 주 양육자의 직접 돌봄이 중요한 아동발달의 특성에 따라 영아기 돌봄을 두텁게 지원하기 위해 부모급여 지급 - 영아수당을 부모급여로 변경‧확대하는 것입니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 부모급여(영아수당)의 2024년 확정된 예산은 몇백만원인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  11%|█         | 11/98 [03:22<29:47, 20.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 2024년 부모급여(영아수당)의 예산은 2,888,694 백만원 입니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 부모급여 지원 사업은 어떤 법령상 근거를 갖고 추진되고 있나요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  12%|█▏        | 12/98 [03:32<24:56, 17.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 부모급여 지원 사업은 **아동수당법 제4조제5항**에 근거하여 추진되고 있습니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 영아수당 도입에 대한 추진경위는 어떻게 되나요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  13%|█▎        | 13/98 [04:39<45:49, 32.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 영아수당은 2022년 1월부터 지원을 시작했고, 2023년 1월부터는 부모급여로 변경되어 지원되고 있습니다. 2023년에는 70만원, 2024년부터는 월 100만원을 지원합니다.  \n",
      "    \n",
      "    추진 경위는 다음과 같습니다:\n",
      "    * 2021년 8월 예비타당성조사 통과\n",
      "    * 2021년 12월 근거법 마련\n",
      "    * 2022년 1월 영아수당 지원사업 시행\n",
      "    * 2022년 5월 '부모급여 도입' 대통령 공약사항 및 국정과제에 포함\n",
      "    * 2023년 5월 ｢아동수당법｣ 개정으로 부모급여 지급 근거 마련\n",
      "    * 2023년 1월 부모급여 지원사업 시행\n",
      "    \n",
      "    사업은 보건복지부, 시·도 및 시·군·구가 주관하며, 만 0~1세 아동에게 지급됩니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 부모급여 지원사업은 언제부터 시행되었나요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  14%|█▍        | 14/98 [04:49<35:47, 25.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 부모급여 지원사업은 2022년 1월부터 시행되었습니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 보건복지부의 부모급여(영아수당) 지원 사업시행방법은 무엇이며, 사업 수혜자는 누구인가?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  15%|█▌        | 15/98 [05:04<30:54, 22.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 보건복지부의 부모급여(영아수당) 지원 사업은 지자체 보조 방식으로 시행됩니다. 사업 수혜자는 만 0~1세 아동입니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 노인장기요양보험 사업 운영에 대한 목적은 무엇인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  16%|█▋        | 16/98 [05:34<33:48, 24.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 노인장기요양보험 사업은 고령이나 노인성 질병으로 일상생활을 혼자서 수행하기 어려운 노인에게 신체 또는 가사 활동 등을 제공하는 노인장기요양보험에 국고지원을 하여, 효율적인 정책추진으로 노후의 건강증진 및 생활 안정을 도모하고 가족의 부담을 완화하여 국민 삶의 질을 향상시키기 위한 사업입니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 노인장기요양보험 운영지원에 대한 사업 내용을 설명해줘.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  17%|█▋        | 17/98 [07:19<1:06:03, 48.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 노인장기요양보험 운영지원 사업은 노인등에게 신체 또는 가사 활동 등을 제공하는 노인장기요양보험에 국고지원을 하여, 효율적인 정책추진으로 노후의 건강증진 및 생활 안정을 도모하고 가족의 부담을 완화하여 국민 삶의 질을 향상시키는 것을 목표로 합니다. \n",
      "\n",
      "    주요 내용은 다음과 같습니다:\n",
      "\n",
      "    * **법정지원금 지원**: 「노인장기요양보험법」 제58조에 따라 국가가 국민건강보험 공단에 장기요양보험료 예상수입액의 20% 상당의 법정지원금을 지원합니다.\n",
      "    * **공무원·사립학교교원 등 장기요양보험료 국가부담금**: 공무원·사립학교 교원의 장기요양 보험료 국가부담분 및 차상위계층의 장기요양보험료 지원을 통해 부담을 완화합니다.\n",
      "    * **기타 의료급여수급권자 급여비용 국가부담금**: 「국민기초생활 보장법」에 의한 의료 급여수급권자를 제외한 기타 의료급여수급권자의 장기요양급여 이용에 따른 급여 비용 및 관리운영비에 대한 국고지원(서울 50%, 기타지역 80%)을 제공합니다.\n",
      "    * **노인장기요양보험 사업관리**: 노인장기요양보험 사업추진에 필요한 경비를 지원합니다.\n",
      "    * **장기요양기관 재무회계프로그램 구축·운영**: 장기요양기관 회계 투명성 확대를 위한 재무회계프로그램 운영에 필요한 운영비 및 인건비를 지원합니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 국고지원을 받는 기타 의료급여수급권자는 누구인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  18%|█▊        | 18/98 [07:39<53:36, 40.20s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 국고지원을 받는 기타 의료급여수급권자는 **의료급여법 제3조제1항제1호에 따른 의료급여를 받는 사람** 이외의 규정에 따른 의료급여를 받는 사람으로,  각 목의 구분에 따라 부담이 됩니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 장기요양보험가입자 및 피부양자의 자격취득과 관련하여 어떤 법률을 준용해야 하는가?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  19%|█▉        | 19/98 [08:21<53:25, 40.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 노인장기요양보험법 제11조(장기요양보험가입자 및 피부양자의 자격취득과 관련하여 어떤 법률을 준용해야 하는가) 에서  국민건강보험법 제5조, 제 6조, 제8조부터 제11조까지, 제69조제1항부터 제3항까지, 제76조부터 제86조까지 및 제110조는 장기요양보험가입자·피부양자의 자격취득·상실, 장기요양보험료 등의 납부·징수 및 결손처분 등에 관하여 이를 준용한다. 라고 명시되어 있습니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 노인장기요양보험법이 언제 제정되고 공포되었나?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  20%|██        | 20/98 [08:33<41:45, 32.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 노인장기요양보험법은 2007년 4월에 제정되고 공포되었습니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 장기요양인정점수 완화가 언제 이루어졌으며, 어떤 변화가 있었나?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  21%|██▏       | 21/98 [09:08<42:11, 32.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 장기요양인정점수 완화는 2012년 7월부터 단계적으로 이루어졌습니다. \n",
      "\n",
      "    * 2012년 7월: 3등급 인정점수 완화: (기존) 55~75점 → (개선) 53점~75점\n",
      "    * 2013년 7월: 3등급 인정점수 완화: (기존) 53~75점 → (개선) 51점~75점 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 장기요양기관 지정갱신제의 법적 근거가 언제 마련되었는가?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  22%|██▏       | 22/98 [09:20<33:45, 26.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 장기요양기관 지정갱신제의 법적 근거가 마련된 시기는 2018년 12월입니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 22.10월에 요양보호사 1명당 시설수급자 인력배치기준이 개선된 내용은 무엇인가?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  23%|██▎       | 23/98 [09:37<29:36, 23.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 2022년 10월 요양보호사 1명당 시설수급자 인력배치기준이 개선되어 요양보호사 1명당 시설수급자 2.3명으로 변경되었습니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 에너지 바우처 제도의 주요 내용은 무엇인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  24%|██▍       | 24/98 [11:14<56:23, 45.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 에너지 바우처 제도는 경제적 어려움으로 에너지 이용에 어려움을 겪는 에너지 소외 계층에게 전기, 가스, 지역난방 등 에너지 이용에 필요한 비용을 지원하는 제도입니다. \n",
      "\n",
      "    이 제도는 하절기 바우처, 동절기 바우처, 연탄 쿠폰, 등유 바우처 등 다양한 형태로 운영되며, 2020년부터 외국인을 포함하여 지원 대상이 확대되고 있습니다. 또한, 주거·교육급여 수급세대 중 더위·추위 민감 계층까지 영구적으로 지원 대상으로 확대되었습니다. \n",
      "\n",
      "    주요 내용은 다음과 같습니다:\n",
      "\n",
      "    * 하절기 바우처: 하절기 냉방 등을 위한 전기 요금을 가상 카드 형태로 지원합니다.\n",
      "    * 동절기 바우처: 동절기 난방 등을 위한 연탄, 등유, LPG, 전기, 도시가스, 지역난방 등 연료비를 가상 카드, 실물 카드 형태로 지원합니다.\n",
      "    * 연탄 쿠폰: 연탄 보일러를 사용하는 가구에 연탄 구입 비용을 연탄 쿠폰 형태로 지원합니다.\n",
      "    * 등유 바우처: 등유 보일러를 사용하는 가구에 동절기 난방을 위한 난방용 등유 구입 비용을 실물 카드 형태로 지원합니다.\n",
      "\n",
      "    이 제도를 통해 저소득층의 에너지 접근성을 높이고, 취약계층의 에너지 비용 부담을 경감하는 효과를 기대할 수 있습니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 에너지바우처 사업의 주요 수혜자는 누구인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  26%|██▌       | 25/98 [12:01<56:11, 46.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 에너지바우처 사업의 주요 수혜자는 노인, 장애인, 영유아, 임산부, 중증·희귀·중증난치질환자, 한부모, 소년소녀가정 등 경제적 부담으로 에너지 이용에 어려움을 겪는 에너지 소외 계층입니다. \n",
      "\n",
      "    구체적으로는 다음과 같은 세대가 포함됩니다.\n",
      "    * 연탄을 사용하는 기초생활수급자, 차상위계층, 기타 소외계층\n",
      "    * 생계·의료급여(중위소득 40%이하) 수급세대 중, 한부모, 소년소녀가정세대 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 2024년 에너지바우처 사업의 사업시행주체는 무엇인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  27%|██▋       | 26/98 [12:21<46:08, 38.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 2024년 에너지바우처 사업의 사업시행주체는 **한국에너지공단**과 **한국광해광업공단** 입니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 하절기바우처와 동절기바우처의 2024년 예산 규모는 각각 얼마인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  28%|██▊       | 27/98 [12:50<42:06, 35.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 하절기바우처 2024년 예산은 60,950백만원입니다.\n",
      "    동절기바우처 2024년 예산은 600,521백만원입니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 2023년 에너지바우처 사업 예산에서 사업운영비 중 에너지복지 홍보에 얼마가 할당되었나요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  29%|██▊       | 28/98 [13:14<37:20, 32.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 2023년 에너지바우처 사업 예산에서 사업운영비 중 에너지복지 홍보에 얼마가 할당되었는지에 대한 답변은 328백만원입니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 2023년 에너지바우처 사업 예산에서 사업운영비 중 시스템 고도화에 얼마가 할당되었나요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  30%|██▉       | 29/98 [13:57<40:44, 35.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 2023년 에너지바우처 사업 예산에서 시스템 고도화에 얼마가 할당되었는지에 대한 답변은 2023년 예산 및 2024년 예산안 산출 세부내역에서 찾을 수 있습니다.\n",
      "\n",
      "    2023년 예산안에서 시스템 고도화 예산은 520백만원입니다. 2024년 예산안에서 시스템 고도화 예산은 705백만원으로 증가합니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 2023년 에너지바우처 사업 예산에서 콜센터 운영에 얼마가 할당되었나요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  31%|███       | 30/98 [14:31<39:41, 35.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 2023년 에너지바우처 사업 예산에서 콜센터 운영에 얼마가 할당되었는지에 대한 정보는 제공된 텍스트에 포함되어 있습니다. \n",
      "\n",
      "    2023년 사업운영비 예산 내역에서 콜센터 운영 비용은 280백만원으로 지정되었습니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 2023년 에너지바우처 사업 예산에서 패널조사에 얼마가 할당되었나요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  32%|███▏      | 31/98 [15:10<40:25, 36.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 2023년 에너지바우처 사업 예산에서 패널조사에 얼마가 할당되었는지에 대한 답변은 2023년 예산 및 2024년 예산 산출 세부내역 비교 부분에서 찾을 수 있습니다. \n",
      "\n",
      "    2023년 예산의 경우 패널조사에 133.5백만원이, 2024년 예산의 경우 131.7백만원이 할당되었습니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 2023년 에너지바우처 사업 예산에서 에너지바우처 전달체계 구축에 얼마가 할당되었나요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  33%|███▎      | 32/98 [15:39<37:14, 33.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 2023년 에너지바우처 사업 예산에서 에너지바우처 전달체계 구축에 얼마가 할당되었나요? 2023년 예산에서 에너지바우처 전달체계 구축에 37.5백만원이 할당되었습니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 2023년 에너지바우처 사업 예산에서 주택관리공단 운영지원에 얼마가 할당되었나요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  34%|███▎      | 33/98 [16:42<46:15, 42.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 2023년 에너지바우처 사업 예산에서 주택관리공단 운영지원에 얼마가 할당되었는지에 대한 답변은 2023년 예산 및 2024년 예산안에서 찾아볼 수 있습니다. \n",
      "\n",
      "    2023년 예산안에서  '○ 주택관리공단 운영지원'은 사업운영비 1,349백만원에 포함되어 있습니다. \n",
      "\n",
      "    2024년 예산안에서도 '사업운영비' 항목에 주택관리공단 운영지원이 포함되어 1,872백만원으로 예산이 확정되었습니다.\n",
      "\n",
      "    따라서 2023년에는 50백만원, 2024년에는 50백만원이 주택관리공단 운영지원에 할당되었습니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 에너지바우처 사업의 향후 기대효과는 무엇인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  35%|███▍      | 34/98 [17:09<40:35, 38.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 에너지바우처 사업의 향후 기대효과는 저소득층의 적정수준 에너지 접근성이 높아지고, 취약계층의 에너지 비용 부담 경감(약 115만 세대, 세대당 평균 36.7만원 내외)을 기대할 수 있습니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 에너지바우처 사업에 대한 예비타당성조사를 어떤 조사기관이 수행했나요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:  36%|███▌      | 35/98 [17:36<36:28, 34.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 에너지바우처 사업에 대한 예비타당성조사를 어떤 조사기관이 수행했나요?\n",
      "\n",
      "    KDI(한국개발연구원 공공투자관리센터)가 2014년도 에너지바우처 지원사업(예비타당성조사 보고서)을 수행했습니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 21년 국정감사에서 에너지 바우처 사업에 대한 주요 지적사항은 무엇이었나요?\n"
     ]
    }
   ],
   "source": [
    "# 결과를 저장할 리스트 초기화\n",
    "results = []\n",
    "\n",
    "# DataFrame의 각 행에 대해 처리\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n",
    "    # 소스 문자열 정규화\n",
    "    source = normalize_string(row['Source'])\n",
    "    question = row['Question']\n",
    "\n",
    "    # 정규화된 키로 데이터베이스 검색\n",
    "    normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n",
    "    retriever = normalized_keys[source]['retriever']\n",
    "    compressor = CrossEncoderReranker(model=model, top_n=5)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=retriever)\n",
    "\n",
    "    template = \"\"\"\n",
    "    {question}\n",
    "\n",
    "    {context}\n",
    "\n",
    "    주어진 질문에만 답변하세요.\n",
    "    \n",
    "    답변:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "   \n",
    "    # RAG 체인 정의\n",
    "    rag_chain = (\n",
    "        {\"context\": compression_retriever | reorder_documents, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 답변 추론\n",
    "    print(f\"Question: {question}\")\n",
    "    full_response = rag_chain.invoke(question)\n",
    "    \n",
    "    print(f\"Answer: {full_response}\\n\")\n",
    "\n",
    "    # 결과 저장\n",
    "    results.append({\n",
    "        \"Source\": row['Source'],\n",
    "        \"Source_path\": row['Source_path'],\n",
    "        \"Question\": question,\n",
    "        \"Answer\": full_response\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f4bd80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m submit_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_directory, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m) )\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 생성된 답변을 제출 DataFrame에 추가\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m submit_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults\u001b[49m]\n\u001b[0;32m      7\u001b[0m submit_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m submit_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m데이콘\u001b[39m\u001b[38;5;124m\"\u001b[39m)     \u001b[38;5;66;03m# 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 결과를 CSV 파일로 저장\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# 제출용 샘플 파일 로드\n",
    "base_directory = \"\"\n",
    "submit_df = pd.read_csv(os.path.join(base_directory, \"sample_submission.csv\") )\n",
    "\n",
    "# 생성된 답변을 제출 DataFrame에 추가\n",
    "submit_df['Answer'] = [item['Answer'] for item in results]\n",
    "submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "submit_df.to_csv(args.output_csv_file, encoding='UTF-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5463022,
     "sourceId": 9155938,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
